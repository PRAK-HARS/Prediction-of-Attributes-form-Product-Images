{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resnet_50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarees attr_4 using Resnet_50 for extracting features and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    'Sarees': ['attr_4'],\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator():\n",
    "    return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define feature extraction function using ResNet50\n",
    "def extract_features(image_files, image_dir, model, target_size=(224, 224), batch_size=32):\n",
    "    data_gen = create_data_generator()\n",
    "    generator = data_gen.flow_from_dataframe(\n",
    "        dataframe=image_files, directory=image_dir, x_col='filename', y_col=None,\n",
    "        target_size=target_size, batch_size=batch_size, class_mode=None, shuffle=False\n",
    "    )\n",
    "    features = model.predict(generator, verbose=1)\n",
    "    return features\n",
    "\n",
    "# Load pre-trained ResNet50 without the fully connected layers, for feature extraction\n",
    "feature_extractor = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n",
    "\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        num_classes = df_attr[attr].nunique()\n",
    "        binary = (num_classes == 2)\n",
    "        \n",
    "        # Use label encoding for all attributes\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_accuracy_across_folds = 0\n",
    "        best_model_path = f'best_knn_model_{category}_{attr}.pkl'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Extract features for train and validation sets\n",
    "            train_features = extract_features(train_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            val_features = extract_features(val_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            \n",
    "            # Train KNN model\n",
    "            knn = KNeighborsClassifier(n_neighbors=7)  # You can adjust the number of neighbors based on experiments\n",
    "            knn.fit(train_features, train_fold['label'])\n",
    "            \n",
    "            # Validate the model\n",
    "            val_preds = knn.predict(val_features)\n",
    "            val_accuracy = accuracy_score(val_fold['label'], val_preds)\n",
    "            print(f\"Validation accuracy for fold {fold+1}: {val_accuracy}\")\n",
    "\n",
    "            # Check if this fold's accuracy is the best so far\n",
    "            if val_accuracy > best_accuracy_across_folds:\n",
    "                best_accuracy_across_folds = val_accuracy\n",
    "                # Save the model here if you want, e.g., using joblib or pickle\n",
    "                # joblib.dump(knn, best_model_path)\n",
    "                print(f\"New best model for {category} - {attr} with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "        print(f\"Best model across all folds for {category} - {attr} with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "# ---- Prediction on Test Set ----\n",
    "# Example prediction flow\n",
    "df_test['filename'] = df_test['id'].astype(str).apply(lambda x: x.zfill(6) + '.jpg')\n",
    "for category, attributes in categories_attributes.items():\n",
    "    for attr in attributes:\n",
    "        print(f\"Predicting on test set for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Load the best model (if saved)\n",
    "        # knn = joblib.load(best_model_path)\n",
    "        \n",
    "        # Extract features for the test set\n",
    "        test_features = extract_features(df_test[['filename']], test_image_dir, feature_extractor)\n",
    "        \n",
    "        # Make predictions\n",
    "        test_preds = knn.predict(test_features)\n",
    "        \n",
    "        # Convert labels back to original categories\n",
    "        test_preds_decoded = label_encoders[category][attr].inverse_transform(test_preds)\n",
    "        df_test[f'predicted_{attr}'] = test_preds_decoded\n",
    "\n",
    "# Save test predictions\n",
    "df_test[['id'] + [f'predicted_{attr}' for attr in categories_attributes['Sarees']]].to_csv('test_predictions.csv', index=False)\n",
    "print(\"Test predictions saved to 'test_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarees attr_5 using Resnet_50 for extracting features and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    #'Men Tshirts': ['attr_1','attr_2', 'attr_3', 'attr_4','attr_5'],\n",
    "    'Sarees': ['attr_5'],\n",
    "    #'Kurtis': [ 'attr_2'],\n",
    "    #'Women Tshirts': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8'],\n",
    "    #'Women Tops and Tunics': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10']\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator(train=True):\n",
    "    if train:\n",
    "        return ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,\n",
    "                                  shear_range=0.2, zoom_range=0.4, horizontal_flip=True, fill_mode='nearest')\n",
    "    else:\n",
    "        return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define model-building function\n",
    "def build_model(num_classes):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "    ])\n",
    "    #for layer in base_model.layers[:]:\n",
    "    #    layer.trainable = True\n",
    "    #for layer in base_model.layers[:-20]:\n",
    "     #   layer.trainable = False   \n",
    "    \n",
    "    # Add L2 regularization with a lambda value (you can adjust the value)\n",
    "    l2_lambda = 0.001  # Example value; you can adjust it based on your experiments\n",
    "    \n",
    "    if num_classes == 2:  # Binary classification\n",
    "        model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    else:  # Multi-class classification with sparse labels\n",
    "       # model.add(Dense(256, activation='relu'))\n",
    "       # model.add(Dropout(0.4))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "# Training process for each category and attribute\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        num_classes = df_attr[attr].nunique()\n",
    "        binary = (num_classes == 2)\n",
    "        \n",
    "        # Use label encoding for all attributes\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "        if binary:\n",
    "            df_attr['label'] = df_attr['label'].astype(str)\n",
    "\n",
    "        # Calculate class weights\n",
    "       # class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(df_attr['label']), y=df_attr['label'])\n",
    "        #class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_accuracy_across_folds = 0\n",
    "        best_model_path = f'best_model_{category}_{attr}.h5'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        best_fold_accuracy = 0\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Create data generators\n",
    "            train_generator = create_data_generator(train=True).flow_from_dataframe(\n",
    "                dataframe=train_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw')\n",
    "\n",
    "            val_generator = create_data_generator(train=False).flow_from_dataframe(\n",
    "                dataframe=val_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw', shuffle=False)\n",
    "\n",
    "            # Build the model\n",
    "            model = build_model(num_classes=num_classes)\n",
    "\n",
    "            # Set up variables to track the best model in this fold\n",
    "            \n",
    "            best_fold_model_path = f'best_model_{category}_{attr}_fold{fold+1}.h5'\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-7, verbose=1)\n",
    "            \n",
    "\n",
    "            # Train for multiple epochs and save the model for the best epoch in this fold\n",
    "            for epoch in range(10):\n",
    "                print(f\"Epoch {epoch+1}/{10}\")\n",
    "                history = model.fit(\n",
    "                    train_generator, validation_data=val_generator, epochs=1, verbose=1,callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "                # Get the validation accuracy of the current epoch\n",
    "                val_accuracy = history.history['val_accuracy'][0]\n",
    "\n",
    "                # If the current epoch's accuracy is the best, save the model for this epoch\n",
    "                if val_accuracy > best_fold_accuracy:\n",
    "                    best_fold_accuracy = val_accuracy\n",
    "                    model.save(best_fold_model_path)\n",
    "                    print(f\"New best model saved for fold {fold+1} at epoch {epoch+1} with accuracy: {val_accuracy}\")\n",
    "\n",
    "            # After the fold, check if this fold's best model is better than previous folds\n",
    "            if best_fold_accuracy > best_accuracy_across_folds:\n",
    "                best_accuracy_across_folds = best_fold_accuracy\n",
    "                best_model_path = best_fold_model_path\n",
    "                print(f\"Best model across all folds so far for {category} - {attr} saved with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "        print(f\"Overall best model for {category} - {attr} saved at: {best_model_path} with accuracy: {best_accuracy_across_folds}\")\n",
    "# ---- Prediction on Test Set ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarees attr_6 using Resnet_50 for extracting features and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    #'Men Tshirts': ['attr_1','attr_2', 'attr_3', 'attr_4','attr_5'],\n",
    "    'Sarees': ['attr_6'],\n",
    "    #'Kurtis': [ 'attr_2'],\n",
    "    #'Women Tshirts': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8'],\n",
    "    #'Women Tops and Tunics': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10']\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator(train=True):\n",
    "    if train:\n",
    "        return ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,\n",
    "                                  shear_range=0.2, zoom_range=0.4, horizontal_flip=True, fill_mode='nearest')\n",
    "    else:\n",
    "        return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define model-building function\n",
    "def build_model(num_classes):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "    ])\n",
    "    #for layer in base_model.layers[:]:\n",
    "    #    layer.trainable = True\n",
    "    for layer in base_model.layers[:-20]:\n",
    "        layer.trainable = False   \n",
    "    \n",
    "    # Add L2 regularization with a lambda value (you can adjust the value)\n",
    "    l2_lambda = 0.001  # Example value; you can adjust it based on your experiments\n",
    "    \n",
    "    if num_classes == 2:  # Binary classification\n",
    "        model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.00001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    else:  # Multi-class classification with sparse labels\n",
    "       # model.add(Dense(256, activation='relu'))\n",
    "       # model.add(Dropout(0.4))\n",
    "       # model.add(Dense(128, activation='relu'))\n",
    "       # model.add(Dropout(0.3))\n",
    "        model.add(Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.00001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "# Training process for each category and attribute\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        num_classes = df_attr[attr].nunique()\n",
    "        binary = (num_classes == 2)\n",
    "        \n",
    "        # Use label encoding for all attributes\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "        if binary:\n",
    "            df_attr['label'] = df_attr['label'].astype(str)\n",
    "\n",
    "        # Calculate class weights\n",
    "       # class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(df_attr['label']), y=df_attr['label'])\n",
    "        #class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_accuracy_across_folds = 0\n",
    "        best_model_path = f'best_model_{category}_{attr}.h5'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        best_fold_accuracy = 0\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Create data generators\n",
    "            train_generator = create_data_generator(train=True).flow_from_dataframe(\n",
    "                dataframe=train_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw')\n",
    "\n",
    "            val_generator = create_data_generator(train=False).flow_from_dataframe(\n",
    "                dataframe=val_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw', shuffle=False)\n",
    "\n",
    "            # Build the model\n",
    "            model = build_model(num_classes=num_classes)\n",
    "\n",
    "            # Set up variables to track the best model in this fold\n",
    "            \n",
    "            best_fold_model_path = f'best_model_{category}_{attr}_fold{fold+1}.h5'\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-7, verbose=1)\n",
    "            \n",
    "\n",
    "            # Train for multiple epochs and save the model for the best epoch in this fold\n",
    "            for epoch in range(10):\n",
    "                print(f\"Epoch {epoch+1}/{10}\")\n",
    "                history = model.fit(\n",
    "                    train_generator, validation_data=val_generator, epochs=1, verbose=1,callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "                # Get the validation accuracy of the current epoch\n",
    "                val_accuracy = history.history['val_accuracy'][0]\n",
    "\n",
    "                # If the current epoch's accuracy is the best, save the model for this epoch\n",
    "                if val_accuracy > best_fold_accuracy:\n",
    "                    best_fold_accuracy = val_accuracy\n",
    "                    model.save(best_fold_model_path)\n",
    "                    print(f\"New best model saved for fold {fold+1} at epoch {epoch+1} with accuracy: {val_accuracy}\")\n",
    "\n",
    "            # After the fold, check if this fold's best model is better than previous folds\n",
    "            if best_fold_accuracy > best_accuracy_across_folds:\n",
    "                best_accuracy_across_folds = best_fold_accuracy\n",
    "                best_model_path = best_fold_model_path\n",
    "                print(f\"Best model across all folds so far for {category} - {attr} saved with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "        print(f\"Overall best model for {category} - {attr} saved at: {best_model_path} with accuracy: {best_accuracy_across_folds}\")\n",
    "# ---- Prediction on Test Set ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarees attr_7 using Resnet_50 for extracting features and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    #'Men Tshirts': ['attr_1','attr_2', 'attr_3', 'attr_4','attr_5'],\n",
    "    'Sarees': ['attr_7'],\n",
    "    #'Kurtis': [ 'attr_2'],\n",
    "    #'Women Tshirts': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8'],\n",
    "    #'Women Tops and Tunics': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10']\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator(train=True):\n",
    "    if train:\n",
    "        return ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,\n",
    "                                  shear_range=0.2, zoom_range=0.4, horizontal_flip=True, fill_mode='nearest')\n",
    "    else:\n",
    "        return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define model-building function\n",
    "def build_model(num_classes):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "    ])\n",
    "    #for layer in base_model.layers[:]:\n",
    "     #   layer.trainable = True\n",
    "    #for layer in base_model.layers[:-20]:\n",
    "    #    layer.trainable = False   \n",
    "    \n",
    "    # Add L2 regularization with a lambda value (you can adjust the value)\n",
    "    l2_lambda = 0.001  # Example value; you can adjust it based on your experiments\n",
    "    \n",
    "    if num_classes == 2:  # Binary classification\n",
    "      #  model.add(Dense(256, activation='relu'))\n",
    "       # model.add(Dropout(0.4))\n",
    "        #model.add(Dense(128, activation='relu'))\n",
    "        #model.add(Dropout(0.3))\n",
    "        model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    else:  # Multi-class classification with sparse labels\n",
    "        #model.add(Dense(256, activation='relu'))\n",
    "        #model.add(Dropout(0.4))\n",
    "        #model.add(Dense(128, activation='relu'))\n",
    "        #model.add(Dropout(0.3))\n",
    "        model.add(Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "# Training process for each category and attribute\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        num_classes = df_attr[attr].nunique()\n",
    "        binary = (num_classes == 2)\n",
    "        \n",
    "        # Use label encoding for all attributes\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "        if binary:\n",
    "            df_attr['label'] = df_attr['label'].astype(str)\n",
    "\n",
    "        # Calculate class weights\n",
    "       # class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(df_attr['label']), y=df_attr['label'])\n",
    "        #class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_accuracy_across_folds = 0\n",
    "        best_model_path = f'best_model_{category}_{attr}.h5'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        best_fold_accuracy = 0\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Create data generators\n",
    "            train_generator = create_data_generator(train=True).flow_from_dataframe(\n",
    "                dataframe=train_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw')\n",
    "\n",
    "            val_generator = create_data_generator(train=False).flow_from_dataframe(\n",
    "                dataframe=val_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw', shuffle=False)\n",
    "\n",
    "            # Build the model\n",
    "            model = build_model(num_classes=num_classes)\n",
    "\n",
    "            # Set up variables to track the best model in this fold\n",
    "            \n",
    "            best_fold_model_path = f'best_model_{category}_{attr}_fold{fold+1}.h5'\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-7, verbose=1)\n",
    "            \n",
    "\n",
    "            # Train for multiple epochs and save the model for the best epoch in this fold\n",
    "            for epoch in range(10):\n",
    "                print(f\"Epoch {epoch+1}/{10}\")\n",
    "                history = model.fit(\n",
    "                    train_generator, validation_data=val_generator, epochs=1, verbose=1,callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "                # Get the validation accuracy of the current epoch\n",
    "                val_accuracy = history.history['val_accuracy'][0]\n",
    "\n",
    "                # If the current epoch's accuracy is the best, save the model for this epoch\n",
    "                if val_accuracy > best_fold_accuracy:\n",
    "                    best_fold_accuracy = val_accuracy\n",
    "                    model.save(best_fold_model_path)\n",
    "                    print(f\"New best model saved for fold {fold+1} at epoch {epoch+1} with accuracy: {val_accuracy}\")\n",
    "\n",
    "            # After the fold, check if this fold's best model is better than previous folds\n",
    "            if best_fold_accuracy > best_accuracy_across_folds:\n",
    "                best_accuracy_across_folds = best_fold_accuracy\n",
    "                best_model_path = best_fold_model_path\n",
    "                print(f\"Best model across all folds so far for {category} - {attr} saved with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "        print(f\"Overall best model for {category} - {attr} saved at: {best_model_path} with accuracy: {best_accuracy_across_folds}\")\n",
    "# ---- Prediction on Test Set ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarees attr_8 using Resnet_50 for extracting features and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    # 'Men Tshirts': ['attr_1','attr_2', 'attr_3', 'attr_4','attr_5'],\n",
    "    'Sarees': ['attr_8'],\n",
    "    # 'Kurtis': [ 'attr_2'],\n",
    "    # 'Women Tshirts': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8'],\n",
    "    # 'Women Tops and Tunics': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10']\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator(train=True):\n",
    "    if train:\n",
    "        return ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,\n",
    "                                  shear_range=0.2, zoom_range=0.4, horizontal_flip=True, fill_mode='nearest')\n",
    "    else:\n",
    "        return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define model-building function\n",
    "def build_model(num_classes):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "    ])\n",
    "    for layer in base_model.layers[:]:\n",
    "        layer.trainable = True\n",
    "    #for layer in base_model.layers[:-20]:\n",
    "    #    layer.trainable = False   \n",
    "    \n",
    "    # Add L2 regularization with a lambda value (you can adjust the value)\n",
    "    l2_lambda = 0.01  # Example value; you can adjust it based on your experiments\n",
    "    \n",
    "    if num_classes == 2:  # Binary classification\n",
    "       # model.add(Dense(256, activation='relu'))\n",
    "        #model.add(Dropout(0.4))\n",
    "        #model.add(Dense(128, activation='relu'))\n",
    "        #model.add(Dropout(0.3))\n",
    "        model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.00001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    else:  # Multi-class classification with sparse labels\n",
    "        #model.add(Dense(256, activation='relu'))\n",
    "        #model.add(Dropout(0.4))\n",
    "        #model.add(Dense(128, activation='relu'))\n",
    "        #model.add(Dropout(0.3))\n",
    "        model.add(Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.00001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "# Training process for each category and attribute\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        num_classes = df_attr[attr].nunique()\n",
    "        binary = (num_classes == 2)\n",
    "        \n",
    "        # Use label encoding for all attributes\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "        if binary:\n",
    "            df_attr['label'] = df_attr['label'].astype(str)\n",
    "\n",
    "        # Calculate class weights\n",
    "       # class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(df_attr['label']), y=df_attr['label'])\n",
    "        #class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_accuracy_across_folds = 0\n",
    "        best_model_path = f'best_model_{category}_{attr}.h5'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        best_fold_accuracy = 0\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Create data generators\n",
    "            train_generator = create_data_generator(train=True).flow_from_dataframe(\n",
    "                dataframe=train_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw')\n",
    "\n",
    "            val_generator = create_data_generator(train=False).flow_from_dataframe(\n",
    "                dataframe=val_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw', shuffle=False)\n",
    "\n",
    "            # Build the model\n",
    "            model = build_model(num_classes=num_classes)\n",
    "\n",
    "            # Set up variables to track the best model in this fold\n",
    "            \n",
    "            best_fold_model_path = f'best_model_{category}_{attr}_fold{fold+1}.h5'\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-7, verbose=1)\n",
    "            \n",
    "\n",
    "            # Train for multiple epochs and save the model for the best epoch in this fold\n",
    "            for epoch in range(10):\n",
    "                print(f\"Epoch {epoch+1}/{10}\")\n",
    "                history = model.fit(\n",
    "                    train_generator, validation_data=val_generator, epochs=1, verbose=1,callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "                # Get the validation accuracy of the current epoch\n",
    "                val_accuracy = history.history['val_accuracy'][0]\n",
    "\n",
    "                # If the current epoch's accuracy is the best, save the model for this epoch\n",
    "                if val_accuracy > best_fold_accuracy:\n",
    "                    best_fold_accuracy = val_accuracy\n",
    "                    model.save(best_fold_model_path)\n",
    "                    print(f\"New best model saved for fold {fold+1} at epoch {epoch+1} with accuracy: {val_accuracy}\")\n",
    "\n",
    "            # After the fold, check if this fold's best model is better than previous folds\n",
    "            if best_fold_accuracy > best_accuracy_across_folds:\n",
    "                best_accuracy_across_folds = best_fold_accuracy\n",
    "                best_model_path = best_fold_model_path\n",
    "                print(f\"Best model across all folds so far for {category} - {attr} saved with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "        print(f\"Overall best model for {category} - {attr} saved at: {best_model_path} with accuracy: {best_accuracy_across_folds}\")\n",
    "# ---- Prediction on Test Set ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarees attr_9 using Resnet_50 for extracting features and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    #'Men Tshirts': ['attr_1','attr_2', 'attr_3', 'attr_4','attr_5'],\n",
    "    'Sarees': ['attr_9'],\n",
    "    #'Kurtis': [ 'attr_2'],\n",
    "    #'Women Tshirts': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8'],\n",
    "    #'Women Tops and Tunics': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10']\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator(train=True):\n",
    "    if train:\n",
    "        return ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,\n",
    "                                  shear_range=0.2, zoom_range=0.4, horizontal_flip=True, fill_mode='nearest')\n",
    "    else:\n",
    "        return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define model-building function\n",
    "def build_model(num_classes):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "    ])\n",
    "    for layer in base_model.layers[:]:\n",
    "        layer.trainable = True\n",
    "    #for layer in base_model.layers[:-20]:\n",
    "    #    layer.trainable = False   \n",
    "    \n",
    "    # Add L2 regularization with a lambda value (you can adjust the value)\n",
    "    l2_lambda = 0.01  # Example value; you can adjust it based on your experiments\n",
    "    \n",
    "    if num_classes == 2:  # Binary classification\n",
    "       # model.add(Dense(256, activation='relu'))\n",
    "        #model.add(Dropout(0.4))\n",
    "        #model.add(Dense(128, activation='relu'))\n",
    "        #model.add(Dropout(0.3))\n",
    "        model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.00001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    else:  # Multi-class classification with sparse labels\n",
    "        #model.add(Dense(256, activation='relu'))\n",
    "        #model.add(Dropout(0.4))\n",
    "        #model.add(Dense(128, activation='relu'))\n",
    "        #model.add(Dropout(0.3))\n",
    "        model.add(Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.00001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "# Training process for each category and attribute\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        num_classes = df_attr[attr].nunique()\n",
    "        binary = (num_classes == 2)\n",
    "        \n",
    "        # Use label encoding for all attributes\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "        if binary:\n",
    "            df_attr['label'] = df_attr['label'].astype(str)\n",
    "\n",
    "        # Calculate class weights\n",
    "       # class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(df_attr['label']), y=df_attr['label'])\n",
    "        #class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_accuracy_across_folds = 0\n",
    "        best_model_path = f'best_model_{category}_{attr}.h5'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        best_fold_accuracy = 0\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Create data generators\n",
    "            train_generator = create_data_generator(train=True).flow_from_dataframe(\n",
    "                dataframe=train_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw')\n",
    "\n",
    "            val_generator = create_data_generator(train=False).flow_from_dataframe(\n",
    "                dataframe=val_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw', shuffle=False)\n",
    "\n",
    "            # Build the model\n",
    "            model = build_model(num_classes=num_classes)\n",
    "\n",
    "            # Set up variables to track the best model in this fold\n",
    "            \n",
    "            best_fold_model_path = f'best_model_{category}_{attr}_fold{fold+1}.h5'\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-7, verbose=1)\n",
    "            \n",
    "\n",
    "            # Train for multiple epochs and save the model for the best epoch in this fold\n",
    "            for epoch in range(10):\n",
    "                print(f\"Epoch {epoch+1}/{10}\")\n",
    "                history = model.fit(\n",
    "                    train_generator, validation_data=val_generator, epochs=1, verbose=1,callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "                # Get the validation accuracy of the current epoch\n",
    "                val_accuracy = history.history['val_accuracy'][0]\n",
    "\n",
    "                # If the current epoch's accuracy is the best, save the model for this epoch\n",
    "                if val_accuracy > best_fold_accuracy:\n",
    "                    best_fold_accuracy = val_accuracy\n",
    "                    model.save(best_fold_model_path)\n",
    "                    print(f\"New best model saved for fold {fold+1} at epoch {epoch+1} with accuracy: {val_accuracy}\")\n",
    "\n",
    "            # After the fold, check if this fold's best model is better than previous folds\n",
    "            if best_fold_accuracy > best_accuracy_across_folds:\n",
    "                best_accuracy_across_folds = best_fold_accuracy\n",
    "                best_model_path = best_fold_model_path\n",
    "                print(f\"Best model across all folds so far for {category} - {attr} saved with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "        print(f\"Overall best model for {category} - {attr} saved at: {best_model_path} with accuracy: {best_accuracy_across_folds}\")\n",
    "# ---- Prediction on Test Set ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarees attr_10 using Resnet_50 for extracting features and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    'Sarees': ['attr_10'],\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator():\n",
    "    return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define feature extraction function using ResNet50\n",
    "def extract_features(image_files, image_dir, model, target_size=(224, 224), batch_size=32):\n",
    "    data_gen = create_data_generator()\n",
    "    generator = data_gen.flow_from_dataframe(\n",
    "        dataframe=image_files, directory=image_dir, x_col='filename', y_col=None,\n",
    "        target_size=target_size, batch_size=batch_size, class_mode=None, shuffle=False\n",
    "    )\n",
    "    features = model.predict(generator, verbose=1)\n",
    "    return features\n",
    "\n",
    "# Load pre-trained ResNet50 without the fully connected layers, for feature extraction\n",
    "feature_extractor = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n",
    "\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        num_classes = df_attr[attr].nunique()\n",
    "        binary = (num_classes == 2)\n",
    "        \n",
    "        # Use label encoding for all attributes\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_accuracy_across_folds = 0\n",
    "        best_model_path = f'best_knn_model_{category}_{attr}.pkl'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Extract features for train and validation sets\n",
    "            train_features = extract_features(train_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            val_features = extract_features(val_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            \n",
    "            # Train KNN model\n",
    "            knn = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors based on experiments\n",
    "            knn.fit(train_features, train_fold['label'])\n",
    "            \n",
    "            # Validate the model\n",
    "            val_preds = knn.predict(val_features)\n",
    "            val_accuracy = accuracy_score(val_fold['label'], val_preds)\n",
    "            print(f\"Validation accuracy for fold {fold+1}: {val_accuracy}\")\n",
    "\n",
    "            # Check if this fold's accuracy is the best so far\n",
    "            if val_accuracy > best_accuracy_across_folds:\n",
    "                best_accuracy_across_folds = val_accuracy\n",
    "                # Save the model here if you want, e.g., using joblib or pickle\n",
    "                # joblib.dump(knn, best_model_path)\n",
    "                print(f\"New best model for {category} - {attr} with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "        print(f\"Best model across all folds for {category} - {attr} with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "# ---- Prediction on Test Set ----\n",
    "# Example prediction flow\n",
    "df_test['filename'] = df_test['id'].astype(str).apply(lambda x: x.zfill(6) + '.jpg')\n",
    "for category, attributes in categories_attributes.items():\n",
    "    for attr in attributes:\n",
    "        print(f\"Predicting on test set for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Load the best model (if saved)\n",
    "        # knn = joblib.load(best_model_path)\n",
    "        \n",
    "        # Extract features for the test set\n",
    "        test_features = extract_features(df_test[['filename']], test_image_dir, feature_extractor)\n",
    "        \n",
    "        # Make predictions\n",
    "        test_preds = knn.predict(test_features)\n",
    "        \n",
    "        # Convert labels back to original categories\n",
    "        test_preds_decoded = label_encoders[category][attr].inverse_transform(test_preds)\n",
    "        df_test[f'predicted_{attr}'] = test_preds_decoded\n",
    "\n",
    "# Save test predictions\n",
    "df_test[['id'] + [f'predicted_{attr}' for attr in categories_attributes['Sarees']]].to_csv('test_predictions.csv', index=False)\n",
    "print(\"Test predictions saved to 'test_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resnet_50 for extracting features and Catboost classifier for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attr_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from catboost import CatBoostClassifier\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    'Sarees': ['attr_5'],\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator():\n",
    "    return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define feature extraction function using ResNet50\n",
    "def extract_features(image_files, image_dir, model, target_size=(224, 224), batch_size=32):\n",
    "    data_gen = create_data_generator()\n",
    "    generator = data_gen.flow_from_dataframe(\n",
    "        dataframe=image_files, directory=image_dir, x_col='filename', y_col=None,\n",
    "        target_size=target_size, batch_size=batch_size, class_mode=None, shuffle=False\n",
    "    )\n",
    "    features = model.predict(generator, verbose=1)\n",
    "    return features\n",
    "\n",
    "# Load pre-trained ResNet50 without the fully connected layers, for feature extraction\n",
    "feature_extractor = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n",
    "\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        \n",
    "        # Encode labels\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_f1_across_folds = 0\n",
    "        best_model_path = f'best_catboost_model_{category}_{attr}.cbm'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Extract features for train and validation sets\n",
    "            train_features = extract_features(train_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            val_features = extract_features(val_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            \n",
    "            # Train CatBoost model with GPU support\n",
    "            catboost_model = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, task_type=\"GPU\", random_seed=42, verbose=100)\n",
    "            catboost_model.fit(train_features, train_fold['label'])\n",
    "            \n",
    "            # Validate the model with adjusted threshold\n",
    "            val_probs = catboost_model.predict_proba(val_features)\n",
    "            \n",
    "            best_f1 = 0\n",
    "            best_threshold = 0.5\n",
    "            for threshold in np.arange(0.1, 0.9, 0.1):\n",
    "                # Apply threshold to probabilities to get class predictions\n",
    "                val_preds = (val_probs >= threshold).argmax(axis=1)\n",
    "                val_f1 = f1_score(val_fold['label'], val_preds, average='weighted')\n",
    "                \n",
    "                if val_f1 > best_f1:\n",
    "                    best_f1 = val_f1\n",
    "                    best_threshold = threshold\n",
    "            \n",
    "            print(f\"Best F1-score for fold {fold+1} with threshold {best_threshold}: {best_f1}\")\n",
    "\n",
    "            # Check if this fold's F1-score is the best so far\n",
    "            if best_f1 > best_f1_across_folds:\n",
    "                best_f1_across_folds = best_f1\n",
    "                # Save the model with the best F1-score across folds\n",
    "                catboost_model.save_model(best_model_path)\n",
    "                print(f\"New best model for {category} - {attr} with F1-score: {best_f1_across_folds}\")\n",
    "\n",
    "        print(f\"Best model across all folds for {category} - {attr} with F1-score: {best_f1_across_folds}\")\n",
    "\n",
    "# ---- Prediction on Test Set ----\n",
    "all_predictions = []\n",
    "\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_test_category = df_test[df_test['Category'] == category]\n",
    "    df_test_category['id'] = df_test_category['id'].astype(str)\n",
    "    df_test_category['filename'] = df_test_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "    \n",
    "    for attr in attributes:\n",
    "        print(f\"Predicting on test set for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Load the best model (if saved)\n",
    "        catboost_model = CatBoostClassifier()\n",
    "        catboost_model.load_model(best_model_path)\n",
    "        \n",
    "        # Extract features for the test set\n",
    "        test_features = extract_features(df_test_category[['filename']], test_image_dir, feature_extractor)\n",
    "        \n",
    "        # Make predictions using best threshold\n",
    "        test_probs = catboost_model.predict_proba(test_features)\n",
    "        test_preds = (test_probs >= best_threshold).argmax(axis=1)\n",
    "        \n",
    "        # Convert labels back to original categories\n",
    "        test_preds_decoded = label_encoders[category][attr].inverse_transform(test_preds)\n",
    "        \n",
    "        # Ensure alignment between predictions and the test dataframe\n",
    "        df_test_category[f'predicted_{attr}'] = test_preds_decoded\n",
    "        \n",
    "        # Append the predictions to the main dataframe\n",
    "        all_predictions.append(df_test_category[['id', f'predicted_{attr}']])\n",
    "\n",
    "# Concatenate all the predictions and save to a CSV\n",
    "df_predictions = pd.concat(all_predictions)\n",
    "df_predictions.to_csv('Sarees_f1_cat_attr_5.csv', index=False)\n",
    "\n",
    "print(\"Test predictions saved to 'Sarees_f1_cat_attr_5.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attr_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from catboost import CatBoostClassifier\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    'Sarees': ['attr_6'],\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator():\n",
    "    return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define feature extraction function using ResNet50\n",
    "def extract_features(image_files, image_dir, model, target_size=(224, 224), batch_size=32):\n",
    "    data_gen = create_data_generator()\n",
    "    generator = data_gen.flow_from_dataframe(\n",
    "        dataframe=image_files, directory=image_dir, x_col='filename', y_col=None,\n",
    "        target_size=target_size, batch_size=batch_size, class_mode=None, shuffle=False\n",
    "    )\n",
    "    features = model.predict(generator, verbose=1)\n",
    "    return features\n",
    "\n",
    "# Load pre-trained ResNet50 without the fully connected layers, for feature extraction\n",
    "feature_extractor = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n",
    "\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        \n",
    "        # Encode labels\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_f1_across_folds = 0\n",
    "        best_model_path = f'best_catboost_model_{category}_{attr}.cbm'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Extract features for train and validation sets\n",
    "            train_features = extract_features(train_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            val_features = extract_features(val_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            \n",
    "            # Train CatBoost model with GPU support\n",
    "            catboost_model = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, task_type=\"GPU\", random_seed=42, verbose=100)\n",
    "            catboost_model.fit(train_features, train_fold['label'])\n",
    "            \n",
    "            # Validate the model with adjusted threshold\n",
    "            val_probs = catboost_model.predict_proba(val_features)\n",
    "            \n",
    "            best_f1 = 0\n",
    "            best_threshold = 0.5\n",
    "            for threshold in np.arange(0.1, 0.9, 0.1):\n",
    "                # Apply threshold to probabilities to get class predictions\n",
    "                val_preds = (val_probs >= threshold).argmax(axis=1)\n",
    "                val_f1 = f1_score(val_fold['label'], val_preds, average='weighted')\n",
    "                \n",
    "                if val_f1 > best_f1:\n",
    "                    best_f1 = val_f1\n",
    "                    best_threshold = threshold\n",
    "            \n",
    "            print(f\"Best F1-score for fold {fold+1} with threshold {best_threshold}: {best_f1}\")\n",
    "\n",
    "            # Check if this fold's F1-score is the best so far\n",
    "            if best_f1 > best_f1_across_folds:\n",
    "                best_f1_across_folds = best_f1\n",
    "                # Save the model with the best F1-score across folds\n",
    "                catboost_model.save_model(best_model_path)\n",
    "                print(f\"New best model for {category} - {attr} with F1-score: {best_f1_across_folds}\")\n",
    "\n",
    "        print(f\"Best model across all folds for {category} - {attr} with F1-score: {best_f1_across_folds}\")\n",
    "\n",
    "# ---- Prediction on Test Set ----\n",
    "all_predictions = []\n",
    "\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_test_category = df_test[df_test['Category'] == category]\n",
    "    df_test_category['id'] = df_test_category['id'].astype(str)\n",
    "    df_test_category['filename'] = df_test_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "    \n",
    "    for attr in attributes:\n",
    "        print(f\"Predicting on test set for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Load the best model (if saved)\n",
    "        catboost_model = CatBoostClassifier()\n",
    "        catboost_model.load_model(best_model_path)\n",
    "        \n",
    "        # Extract features for the test set\n",
    "        test_features = extract_features(df_test_category[['filename']], test_image_dir, feature_extractor)\n",
    "        \n",
    "        # Make predictions using best threshold\n",
    "        test_probs = catboost_model.predict_proba(test_features)\n",
    "        test_preds = (test_probs >= best_threshold).argmax(axis=1)\n",
    "        \n",
    "        # Convert labels back to original categories\n",
    "        test_preds_decoded = label_encoders[category][attr].inverse_transform(test_preds)\n",
    "        \n",
    "        # Ensure alignment between predictions and the test dataframe\n",
    "        df_test_category[f'predicted_{attr}'] = test_preds_decoded\n",
    "        \n",
    "        # Append the predictions to the main dataframe\n",
    "        all_predictions.append(df_test_category[['id', f'predicted_{attr}']])\n",
    "\n",
    "# Concatenate all the predictions and save to a CSV\n",
    "df_predictions = pd.concat(all_predictions)\n",
    "df_predictions.to_csv('Sarees_f1_cat_attr_6.csv', index=False)\n",
    "\n",
    "print(\"Test predictions saved to 'Sarees_f1_cat_attr_6.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attr_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from catboost import CatBoostClassifier\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    'Sarees': ['attr_7'],\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator():\n",
    "    return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define feature extraction function using ResNet50\n",
    "def extract_features(image_files, image_dir, model, target_size=(224, 224), batch_size=32):\n",
    "    data_gen = create_data_generator()\n",
    "    generator = data_gen.flow_from_dataframe(\n",
    "        dataframe=image_files, directory=image_dir, x_col='filename', y_col=None,\n",
    "        target_size=target_size, batch_size=batch_size, class_mode=None, shuffle=False\n",
    "    )\n",
    "    features = model.predict(generator, verbose=1)\n",
    "    return features\n",
    "\n",
    "# Load pre-trained ResNet50 without the fully connected layers, for feature extraction\n",
    "feature_extractor = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n",
    "\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        \n",
    "        # Encode labels\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_f1_across_folds = 0\n",
    "        best_model_path = f'best_catboost_model_{category}_{attr}.cbm'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Extract features for train and validation sets\n",
    "            train_features = extract_features(train_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            val_features = extract_features(val_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            \n",
    "            # Train CatBoost model with GPU support\n",
    "            catboost_model = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, task_type=\"GPU\", random_seed=42, verbose=100)\n",
    "            catboost_model.fit(train_features, train_fold['label'])\n",
    "            \n",
    "            # Validate the model with adjusted threshold\n",
    "            val_probs = catboost_model.predict_proba(val_features)\n",
    "            \n",
    "            best_f1 = 0\n",
    "            best_threshold = 0.5\n",
    "            for threshold in np.arange(0.1, 0.9, 0.1):\n",
    "                # Apply threshold to probabilities to get class predictions\n",
    "                val_preds = (val_probs >= threshold).argmax(axis=1)\n",
    "                val_f1 = f1_score(val_fold['label'], val_preds, average='weighted')\n",
    "                \n",
    "                if val_f1 > best_f1:\n",
    "                    best_f1 = val_f1\n",
    "                    best_threshold = threshold\n",
    "            \n",
    "            print(f\"Best F1-score for fold {fold+1} with threshold {best_threshold}: {best_f1}\")\n",
    "\n",
    "            # Check if this fold's F1-score is the best so far\n",
    "            if best_f1 > best_f1_across_folds:\n",
    "                best_f1_across_folds = best_f1\n",
    "                # Save the model with the best F1-score across folds\n",
    "                catboost_model.save_model(best_model_path)\n",
    "                print(f\"New best model for {category} - {attr} with F1-score: {best_f1_across_folds}\")\n",
    "\n",
    "        print(f\"Best model across all folds for {category} - {attr} with F1-score: {best_f1_across_folds}\")\n",
    "\n",
    "# ---- Prediction on Test Set ----\n",
    "all_predictions = []\n",
    "\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_test_category = df_test[df_test['Category'] == category]\n",
    "    df_test_category['id'] = df_test_category['id'].astype(str)\n",
    "    df_test_category['filename'] = df_test_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "    \n",
    "    for attr in attributes:\n",
    "        print(f\"Predicting on test set for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Load the best model (if saved)\n",
    "        catboost_model = CatBoostClassifier()\n",
    "        catboost_model.load_model(best_model_path)\n",
    "        \n",
    "        # Extract features for the test set\n",
    "        test_features = extract_features(df_test_category[['filename']], test_image_dir, feature_extractor)\n",
    "        \n",
    "        # Make predictions using best threshold\n",
    "        test_probs = catboost_model.predict_proba(test_features)\n",
    "        test_preds = (test_probs >= best_threshold).argmax(axis=1)\n",
    "        \n",
    "        # Convert labels back to original categories\n",
    "        test_preds_decoded = label_encoders[category][attr].inverse_transform(test_preds)\n",
    "        \n",
    "        # Ensure alignment between predictions and the test dataframe\n",
    "        df_test_category[f'predicted_{attr}'] = test_preds_decoded\n",
    "        \n",
    "        # Append the predictions to the main dataframe\n",
    "        all_predictions.append(df_test_category[['id', f'predicted_{attr}']])\n",
    "\n",
    "# Concatenate all the predictions and save to a CSV\n",
    "df_predictions = pd.concat(all_predictions)\n",
    "df_predictions.to_csv('Sarees_f1_cat_attr_7.csv', index=False)\n",
    "\n",
    "print(\"Test predictions saved to 'Sarees_f1_cat_attr_7.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attr_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from catboost import CatBoostClassifier\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    'Sarees': ['attr_8'],\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator():\n",
    "    return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define feature extraction function using ResNet50\n",
    "def extract_features(image_files, image_dir, model, target_size=(224, 224), batch_size=32):\n",
    "    data_gen = create_data_generator()\n",
    "    generator = data_gen.flow_from_dataframe(\n",
    "        dataframe=image_files, directory=image_dir, x_col='filename', y_col=None,\n",
    "        target_size=target_size, batch_size=batch_size, class_mode=None, shuffle=False\n",
    "    )\n",
    "    features = model.predict(generator, verbose=1)\n",
    "    return features\n",
    "\n",
    "# Load pre-trained ResNet50 without the fully connected layers, for feature extraction\n",
    "feature_extractor = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n",
    "\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        \n",
    "        # Encode labels\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_f1_across_folds = 0\n",
    "        best_model_path = f'best_catboost_model_{category}_{attr}.cbm'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Extract features for train and validation sets\n",
    "            train_features = extract_features(train_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            val_features = extract_features(val_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            \n",
    "            # Train CatBoost model with GPU support\n",
    "            catboost_model = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, task_type=\"GPU\", random_seed=42, verbose=100)\n",
    "            catboost_model.fit(train_features, train_fold['label'])\n",
    "            \n",
    "            # Validate the model with adjusted threshold\n",
    "            val_probs = catboost_model.predict_proba(val_features)\n",
    "            \n",
    "            best_f1 = 0\n",
    "            best_threshold = 0.5\n",
    "            for threshold in np.arange(0.1, 0.9, 0.1):\n",
    "                # Apply threshold to probabilities to get class predictions\n",
    "                val_preds = (val_probs >= threshold).argmax(axis=1)\n",
    "                val_f1 = f1_score(val_fold['label'], val_preds, average='weighted')\n",
    "                \n",
    "                if val_f1 > best_f1:\n",
    "                    best_f1 = val_f1\n",
    "                    best_threshold = threshold\n",
    "            \n",
    "            print(f\"Best F1-score for fold {fold+1} with threshold {best_threshold}: {best_f1}\")\n",
    "\n",
    "            # Check if this fold's F1-score is the best so far\n",
    "            if best_f1 > best_f1_across_folds:\n",
    "                best_f1_across_folds = best_f1\n",
    "                # Save the model with the best F1-score across folds\n",
    "                catboost_model.save_model(best_model_path)\n",
    "                print(f\"New best model for {category} - {attr} with F1-score: {best_f1_across_folds}\")\n",
    "\n",
    "        print(f\"Best model across all folds for {category} - {attr} with F1-score: {best_f1_across_folds}\")\n",
    "\n",
    "# ---- Prediction on Test Set ----\n",
    "all_predictions = []\n",
    "\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_test_category = df_test[df_test['Category'] == category]\n",
    "    df_test_category['id'] = df_test_category['id'].astype(str)\n",
    "    df_test_category['filename'] = df_test_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "    \n",
    "    for attr in attributes:\n",
    "        print(f\"Predicting on test set for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Load the best model (if saved)\n",
    "        catboost_model = CatBoostClassifier()\n",
    "        catboost_model.load_model(best_model_path)\n",
    "        \n",
    "        # Extract features for the test set\n",
    "        test_features = extract_features(df_test_category[['filename']], test_image_dir, feature_extractor)\n",
    "        \n",
    "        # Make predictions using best threshold\n",
    "        test_probs = catboost_model.predict_proba(test_features)\n",
    "        test_preds = (test_probs >= best_threshold).argmax(axis=1)\n",
    "        \n",
    "        # Convert labels back to original categories\n",
    "        test_preds_decoded = label_encoders[category][attr].inverse_transform(test_preds)\n",
    "        \n",
    "        # Ensure alignment between predictions and the test dataframe\n",
    "        df_test_category[f'predicted_{attr}'] = test_preds_decoded\n",
    "        \n",
    "        # Append the predictions to the main dataframe\n",
    "        all_predictions.append(df_test_category[['id', f'predicted_{attr}']])\n",
    "\n",
    "# Concatenate all the predictions and save to a CSV\n",
    "df_predictions = pd.concat(all_predictions)\n",
    "df_predictions.to_csv('Sarees_f1_cat_attr_8.csv', index=False)\n",
    "\n",
    "print(\"Test predictions saved to 'Sarees_f1_cat_attr_8.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resnet_50 for extracting features and Random Forest for predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attr_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    'Sarees': ['attr_6'],\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator():\n",
    "    return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define feature extraction function using ResNet50\n",
    "def extract_features(image_files, image_dir, model, target_size=(224, 224), batch_size=32):\n",
    "    data_gen = create_data_generator()\n",
    "    generator = data_gen.flow_from_dataframe(\n",
    "        dataframe=image_files, directory=image_dir, x_col='filename', y_col=None,\n",
    "        target_size=target_size, batch_size=batch_size, class_mode=None, shuffle=False\n",
    "    )\n",
    "    features = model.predict(generator, verbose=1)\n",
    "    return features\n",
    "\n",
    "# Load pre-trained ResNet50 without the fully connected layers, for feature extraction\n",
    "feature_extractor = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n",
    "\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        \n",
    "        # Encode labels\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_f1_across_folds = 0\n",
    "        best_model_path = f'best_rf_model_{category}_{attr}.pkl'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Extract features for train and validation sets\n",
    "            train_features = extract_features(train_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            val_features = extract_features(val_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            \n",
    "            # Train RandomForest model\n",
    "            rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "            rf_model.fit(train_features, train_fold['label'])\n",
    "            \n",
    "            # Validate the model with adjusted threshold\n",
    "            val_probs = rf_model.predict_proba(val_features)\n",
    "            \n",
    "            best_f1 = 0\n",
    "            best_threshold = 0.5\n",
    "            for threshold in np.arange(0.1, 0.9, 0.1):\n",
    "                # Apply threshold to probabilities to get class predictions\n",
    "                val_preds = (val_probs >= threshold).argmax(axis=1)\n",
    "                val_f1 = f1_score(val_fold['label'], val_preds, average='weighted')\n",
    "                \n",
    "                if val_f1 > best_f1:\n",
    "                    best_f1 = val_f1\n",
    "                    best_threshold = threshold\n",
    "            \n",
    "            print(f\"Best F1-score for fold {fold+1} with threshold {best_threshold}: {best_f1}\")\n",
    "\n",
    "            # Check if this fold's F1-score is the best so far\n",
    "            if best_f1 > best_f1_across_folds:\n",
    "                best_f1_across_folds = best_f1\n",
    "                # Save the model with the best F1-score across folds\n",
    "                with open(best_model_path, 'wb') as f:\n",
    "                    pickle.dump(rf_model, f)\n",
    "                print(f\"New best model for {category} - {attr} with F1-score: {best_f1_across_folds}\")\n",
    "\n",
    "        print(f\"Best model across all folds for {category} - {attr} with F1-score: {best_f1_across_folds}\")\n",
    "\n",
    "# ---- Prediction on Test Set ----\n",
    "all_predictions = []\n",
    "\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_test_category = df_test[df_test['Category'] == category]\n",
    "    df_test_category['id'] = df_test_category['id'].astype(str)\n",
    "    df_test_category['filename'] = df_test_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "    \n",
    "    for attr in attributes:\n",
    "        print(f\"Predicting on test set for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Load the best model (if saved)\n",
    "        with open(best_model_path, 'rb') as f:\n",
    "            rf_model = pickle.load(f)\n",
    "        \n",
    "        # Extract features for the test set\n",
    "        test_features = extract_features(df_test_category[['filename']], test_image_dir, feature_extractor)\n",
    "        \n",
    "        # Make predictions using best threshold\n",
    "        test_probs = rf_model.predict_proba(test_features)\n",
    "        test_preds = (test_probs >= best_threshold).argmax(axis=1)\n",
    "        \n",
    "        # Convert labels back to original categories\n",
    "        test_preds_decoded = label_encoders[category][attr].inverse_transform(test_preds)\n",
    "        \n",
    "        # Ensure alignment between predictions and the test dataframe\n",
    "        df_test_category[f'predicted_{attr}'] = test_preds_decoded\n",
    "        \n",
    "        # Append the predictions to the main dataframe\n",
    "        all_predictions.append(df_test_category[['id', f'predicted_{attr}']])\n",
    "\n",
    "# Concatenate all the predictions and save to a CSV\n",
    "df_predictions = pd.concat(all_predictions)\n",
    "df_predictions.to_csv('Sarees_f1_rf_attr_6.csv', index=False)\n",
    "\n",
    "print(\"Test predictions saved to 'Sarees_f1_rf_attr_6.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attr_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    'Sarees': ['attr_7'],\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator():\n",
    "    return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define feature extraction function using ResNet50\n",
    "def extract_features(image_files, image_dir, model, target_size=(224, 224), batch_size=32):\n",
    "    data_gen = create_data_generator()\n",
    "    generator = data_gen.flow_from_dataframe(\n",
    "        dataframe=image_files, directory=image_dir, x_col='filename', y_col=None,\n",
    "        target_size=target_size, batch_size=batch_size, class_mode=None, shuffle=False\n",
    "    )\n",
    "    features = model.predict(generator, verbose=1)\n",
    "    return features\n",
    "\n",
    "# Load pre-trained ResNet50 without the fully connected layers, for feature extraction\n",
    "feature_extractor = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n",
    "\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        \n",
    "        # Encode labels\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_f1_across_folds = 0\n",
    "        best_model_path = f'best_rf_model_{category}_{attr}.pkl'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Extract features for train and validation sets\n",
    "            train_features = extract_features(train_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            val_features = extract_features(val_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            \n",
    "            # Train RandomForest model\n",
    "            rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "            rf_model.fit(train_features, train_fold['label'])\n",
    "            \n",
    "            # Validate the model with adjusted threshold\n",
    "            val_probs = rf_model.predict_proba(val_features)\n",
    "            \n",
    "            best_f1 = 0\n",
    "            best_threshold = 0.5\n",
    "            for threshold in np.arange(0.1, 0.9, 0.1):\n",
    "                # Apply threshold to probabilities to get class predictions\n",
    "                val_preds = (val_probs >= threshold).argmax(axis=1)\n",
    "                val_f1 = f1_score(val_fold['label'], val_preds, average='weighted')\n",
    "                \n",
    "                if val_f1 > best_f1:\n",
    "                    best_f1 = val_f1\n",
    "                    best_threshold = threshold\n",
    "            \n",
    "            print(f\"Best F1-score for fold {fold+1} with threshold {best_threshold}: {best_f1}\")\n",
    "\n",
    "            # Check if this fold's F1-score is the best so far\n",
    "            if best_f1 > best_f1_across_folds:\n",
    "                best_f1_across_folds = best_f1\n",
    "                # Save the model with the best F1-score across folds\n",
    "                with open(best_model_path, 'wb') as f:\n",
    "                    pickle.dump(rf_model, f)\n",
    "                print(f\"New best model for {category} - {attr} with F1-score: {best_f1_across_folds}\")\n",
    "\n",
    "        print(f\"Best model across all folds for {category} - {attr} with F1-score: {best_f1_across_folds}\")\n",
    "\n",
    "# ---- Prediction on Test Set ----\n",
    "all_predictions = []\n",
    "\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_test_category = df_test[df_test['Category'] == category]\n",
    "    df_test_category['id'] = df_test_category['id'].astype(str)\n",
    "    df_test_category['filename'] = df_test_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "    \n",
    "    for attr in attributes:\n",
    "        print(f\"Predicting on test set for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Load the best model (if saved)\n",
    "        with open(best_model_path, 'rb') as f:\n",
    "            rf_model = pickle.load(f)\n",
    "        \n",
    "        # Extract features for the test set\n",
    "        test_features = extract_features(df_test_category[['filename']], test_image_dir, feature_extractor)\n",
    "        \n",
    "        # Make predictions using best threshold\n",
    "        test_probs = rf_model.predict_proba(test_features)\n",
    "        test_preds = (test_probs >= best_threshold).argmax(axis=1)\n",
    "        \n",
    "        # Convert labels back to original categories\n",
    "        test_preds_decoded = label_encoders[category][attr].inverse_transform(test_preds)\n",
    "        \n",
    "        # Ensure alignment between predictions and the test dataframe\n",
    "        df_test_category[f'predicted_{attr}'] = test_preds_decoded\n",
    "        \n",
    "        # Append the predictions to the main dataframe\n",
    "        all_predictions.append(df_test_category[['id', f'predicted_{attr}']])\n",
    "\n",
    "# Concatenate all the predictions and save to a CSV\n",
    "df_predictions = pd.concat(all_predictions)\n",
    "df_predictions.to_csv('Sarees_f1_rf_attr_7.csv', index=False)\n",
    "\n",
    "print(\"Test predictions saved to 'Sarees_f1_rf_attr_7.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
