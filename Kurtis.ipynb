{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resnet_50 for extracting features and predicting attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attr_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    #'Men Tshirts': ['attr_1','attr_2', 'attr_3', 'attr_4','attr_5'],\n",
    "    #'Sarees': ['attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10'],\n",
    "    'Kurtis': ['attr_1'],\n",
    "    #'Women Tshirts': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8'],\n",
    "    #'Women Tops and Tunics': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10']\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator(train=True):\n",
    "    if train:\n",
    "        return ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,\n",
    "                                  shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
    "    else:\n",
    "        return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define model-building function\n",
    "def build_model(num_classes):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "    ])\n",
    "    \n",
    "    # Add L2 regularization with a lambda value (you can adjust the value)\n",
    "    l2_lambda = 0.01  # Example value; you can adjust it based on your experiments\n",
    "    \n",
    "    if num_classes == 2:  # Binary classification\n",
    "        model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    else:  # Multi-class classification with sparse labels\n",
    "        model.add(Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "# Training process for each category and attribute\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        num_classes = df_attr[attr].nunique()\n",
    "        binary = (num_classes == 2)\n",
    "        \n",
    "        # Use label encoding for all attributes\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "        if binary:\n",
    "            df_attr['label'] = df_attr['label'].astype(str)\n",
    "\n",
    "        # Calculate class weights\n",
    "       # class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(df_attr['label']), y=df_attr['label'])\n",
    "        #class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_accuracy_across_folds = 0\n",
    "        best_model_path = f'best_model_{category}_{attr}.h5'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        best_fold_accuracy = 0\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Create data generators\n",
    "            train_generator = create_data_generator(train=True).flow_from_dataframe(\n",
    "                dataframe=train_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw')\n",
    "\n",
    "            val_generator = create_data_generator(train=False).flow_from_dataframe(\n",
    "                dataframe=val_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw', shuffle=False)\n",
    "\n",
    "            # Build the model\n",
    "            model = build_model(num_classes=num_classes)\n",
    "\n",
    "            # Set up variables to track the best model in this fold\n",
    "            \n",
    "            best_fold_model_path = f'best_model_{category}_{attr}_fold{fold+1}.h5'\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-7, verbose=1)\n",
    "            \n",
    "\n",
    "            # Train for multiple epochs and save the model for the best epoch in this fold\n",
    "            for epoch in range(10):\n",
    "                print(f\"Epoch {epoch+1}/{10}\")\n",
    "                history = model.fit(\n",
    "                    train_generator, validation_data=val_generator, epochs=1, verbose=1,callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "                # Get the validation accuracy of the current epoch\n",
    "                val_accuracy = history.history['val_accuracy'][0]\n",
    "\n",
    "                # If the current epoch's accuracy is the best, save the model for this epoch\n",
    "                if val_accuracy > best_fold_accuracy:\n",
    "                    best_fold_accuracy = val_accuracy\n",
    "                    model.save(best_fold_model_path)\n",
    "                    print(f\"New best model saved for fold {fold+1} at epoch {epoch+1} with accuracy: {val_accuracy}\")\n",
    "\n",
    "            # After the fold, check if this fold's best model is better than previous folds\n",
    "            if best_fold_accuracy > best_accuracy_across_folds:\n",
    "                best_accuracy_across_folds = best_fold_accuracy\n",
    "                best_model_path = best_fold_model_path\n",
    "                print(f\"Best model across all folds so far for {category} - {attr} saved with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "        print(f\"Overall best model for {category} - {attr} saved at: {best_model_path} with accuracy: {best_accuracy_across_folds}\")\n",
    "# ---- Prediction on Test Set ----\n",
    "test_df = df_test[['id', 'Category']].copy()\n",
    "test_df['id'] = test_df['id'].astype(str)\n",
    "test_df['filename'] = test_df['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_test_category = test_df[test_df['Category'] == category]\n",
    "\n",
    "    for attr in attributes:\n",
    "        model_path = f'best_model_{category}_{attr}.h5'\n",
    "        model = load_model(model_path)\n",
    "        \n",
    "        test_generator = create_data_generator(train=False).flow_from_dataframe(\n",
    "            dataframe=df_test_category, directory=test_image_dir, x_col='filename', y_col=None,\n",
    "            target_size=(224, 224), batch_size=32, class_mode=None, shuffle=False)\n",
    "        \n",
    "        predictions = model.predict(test_generator)\n",
    "        \n",
    "        # Decode predictions based on encoding type\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        decoded_predictions = label_encoders[category][attr].inverse_transform(predicted_classes)\n",
    "\n",
    "        all_predictions.append(pd.DataFrame({\n",
    "            'filename': df_test_category['filename'], f'{attr}_pred': decoded_predictions\n",
    "        }))\n",
    "\n",
    "# Merge all attribute predictions and save\n",
    "submission_df = pd.concat(all_predictions, axis=1)\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file created: submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attr_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    #'Men Tshirts': ['attr_1','attr_2', 'attr_3', 'attr_4','attr_5'],\n",
    "    #'Sarees': ['attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10'],\n",
    "    'Kurtis': [ 'attr_2'],\n",
    "    #'Women Tshirts': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8'],\n",
    "    #'Women Tops and Tunics': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10']\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator(train=True):\n",
    "    if train:\n",
    "        return ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,\n",
    "                                  shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
    "    else:\n",
    "        return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define model-building function\n",
    "def build_model(num_classes):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "    ])\n",
    "    for layer in base_model.layers[-10:]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # Add L2 regularization with a lambda value (you can adjust the value)\n",
    "    l2_lambda = 0.01  # Example value; you can adjust it based on your experiments\n",
    "    \n",
    "    if num_classes == 2:  # Binary classification\n",
    "        model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.000008), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    else:  # Multi-class classification with sparse labels\n",
    "        model.add(Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.000008), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "# Training process for each category and attribute\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        num_classes = df_attr[attr].nunique()\n",
    "        binary = (num_classes == 2)\n",
    "        \n",
    "        # Use label encoding for all attributes\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "        if binary:\n",
    "            df_attr['label'] = df_attr['label'].astype(str)\n",
    "\n",
    "        # Calculate class weights\n",
    "       # class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(df_attr['label']), y=df_attr['label'])\n",
    "        #class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_accuracy_across_folds = 0\n",
    "        best_model_path = f'best_model_{category}_{attr}.h5'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        best_fold_accuracy = 0\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Create data generators\n",
    "            train_generator = create_data_generator(train=True).flow_from_dataframe(\n",
    "                dataframe=train_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw')\n",
    "\n",
    "            val_generator = create_data_generator(train=False).flow_from_dataframe(\n",
    "                dataframe=val_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw', shuffle=False)\n",
    "\n",
    "            # Build the model\n",
    "            model = build_model(num_classes=num_classes)\n",
    "\n",
    "            # Set up variables to track the best model in this fold\n",
    "            \n",
    "            best_fold_model_path = f'best_model_{category}_{attr}_fold{fold+1}.h5'\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-7, verbose=1)\n",
    "            \n",
    "\n",
    "            # Train for multiple epochs and save the model for the best epoch in this fold\n",
    "            for epoch in range(10):\n",
    "                print(f\"Epoch {epoch+1}/{10}\")\n",
    "                history = model.fit(\n",
    "                    train_generator, validation_data=val_generator, epochs=1, verbose=1,callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "                # Get the validation accuracy of the current epoch\n",
    "                val_accuracy = history.history['val_accuracy'][0]\n",
    "\n",
    "                # If the current epoch's accuracy is the best, save the model for this epoch\n",
    "                if val_accuracy > best_fold_accuracy:\n",
    "                    best_fold_accuracy = val_accuracy\n",
    "                    model.save(best_fold_model_path)\n",
    "                    print(f\"New best model saved for fold {fold+1} at epoch {epoch+1} with accuracy: {val_accuracy}\")\n",
    "\n",
    "            # After the fold, check if this fold's best model is better than previous folds\n",
    "            if best_fold_accuracy > best_accuracy_across_folds:\n",
    "                best_accuracy_across_folds = best_fold_accuracy\n",
    "                best_model_path = best_fold_model_path\n",
    "                print(f\"Best model across all folds so far for {category} - {attr} saved with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "        print(f\"Overall best model for {category} - {attr} saved at: {best_model_path} with accuracy: {best_accuracy_across_folds}\")\n",
    "# ---- Prediction on Test Set ----\n",
    "test_df = df_test[['id', 'Category']].copy()\n",
    "test_df['id'] = test_df['id'].astype(str)\n",
    "test_df['filename'] = test_df['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_test_category = test_df[test_df['Category'] == category]\n",
    "\n",
    "    for attr in attributes:\n",
    "        model_path = f'best_model_{category}_{attr}.h5'\n",
    "        model = load_model(model_path)\n",
    "        \n",
    "        test_generator = create_data_generator(train=False).flow_from_dataframe(\n",
    "            dataframe=df_test_category, directory=test_image_dir, x_col='filename', y_col=None,\n",
    "            target_size=(224, 224), batch_size=32, class_mode=None, shuffle=False)\n",
    "        \n",
    "        predictions = model.predict(test_generator)\n",
    "        \n",
    "        # Decode predictions based on encoding type\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        decoded_predictions = label_encoders[category][attr].inverse_transform(predicted_classes)\n",
    "\n",
    "        all_predictions.append(pd.DataFrame({\n",
    "            'filename': df_test_category['filename'], f'{attr}_pred': decoded_predictions\n",
    "        }))\n",
    "\n",
    "# Merge all attribute predictions and save\n",
    "submission_df = pd.concat(all_predictions, axis=1)\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file created: submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attr_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    #'Men Tshirts': ['attr_1','attr_2', 'attr_3', 'attr_4','attr_5'],\n",
    "    #'Sarees': ['attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10'],\n",
    "    'Kurtis': [ 'attr_3'],\n",
    "    #'Women Tshirts': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8'],\n",
    "    #'Women Tops and Tunics': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10']\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator(train=True):\n",
    "    if train:\n",
    "        return ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,\n",
    "                                  shear_range=0.2, zoom_range=0.4, horizontal_flip=True, fill_mode='nearest')\n",
    "    else:\n",
    "        return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define model-building function\n",
    "def build_model(num_classes):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dropout(0.4),\n",
    "    ])\n",
    "    for layer in base_model.layers[-30:]:\n",
    "        layer.trainable = True\n",
    "    for layer in base_model.layers[:30]:  # Adjust this value to freeze more or fewer layers\n",
    "        layer.trainable = False\n",
    "    # Add L2 regularization with a lambda value (you can adjust the value)\n",
    "    l2_lambda = 0.001  # Example value; you can adjust it based on your experiments\n",
    "    \n",
    "    if num_classes == 2:  # Binary classification\n",
    "        model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.00001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    else:  # Multi-class classification with sparse labels\n",
    "        model.add(Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.00001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "# Training process for each category and attribute\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        num_classes = df_attr[attr].nunique()\n",
    "        binary = (num_classes == 2)\n",
    "        \n",
    "        # Use label encoding for all attributes\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "        if binary:\n",
    "            df_attr['label'] = df_attr['label'].astype(str)\n",
    "\n",
    "        # Calculate class weights\n",
    "       # class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(df_attr['label']), y=df_attr['label'])\n",
    "        #class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_accuracy_across_folds = 0\n",
    "        best_model_path = f'best_model_{category}_{attr}.h5'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        best_fold_accuracy = 0\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Create data generators\n",
    "            train_generator = create_data_generator(train=True).flow_from_dataframe(\n",
    "                dataframe=train_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw')\n",
    "\n",
    "            val_generator = create_data_generator(train=False).flow_from_dataframe(\n",
    "                dataframe=val_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw', shuffle=False)\n",
    "\n",
    "            # Build the model\n",
    "            model = build_model(num_classes=num_classes)\n",
    "\n",
    "            # Set up variables to track the best model in this fold\n",
    "            \n",
    "            best_fold_model_path = f'best_model_{category}_{attr}_fold{fold+1}.h5'\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-7, verbose=1)\n",
    "            \n",
    "\n",
    "            # Train for multiple epochs and save the model for the best epoch in this fold\n",
    "            for epoch in range(10):\n",
    "                print(f\"Epoch {epoch+1}/{10}\")\n",
    "                history = model.fit(\n",
    "                    train_generator, validation_data=val_generator, epochs=1, verbose=1,callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "                # Get the validation accuracy of the current epoch\n",
    "                val_accuracy = history.history['val_accuracy'][0]\n",
    "\n",
    "                # If the current epoch's accuracy is the best, save the model for this epoch\n",
    "                if val_accuracy > best_fold_accuracy:\n",
    "                    best_fold_accuracy = val_accuracy\n",
    "                    model.save(best_fold_model_path)\n",
    "                    print(f\"New best model saved for fold {fold+1} at epoch {epoch+1} with accuracy: {val_accuracy}\")\n",
    "\n",
    "            # After the fold, check if this fold's best model is better than previous folds\n",
    "            if best_fold_accuracy > best_accuracy_across_folds:\n",
    "                best_accuracy_across_folds = best_fold_accuracy\n",
    "                best_model_path = best_fold_model_path\n",
    "                print(f\"Best model across all folds so far for {category} - {attr} saved with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "        print(f\"Overall best model for {category} - {attr} saved at: {best_model_path} with accuracy: {best_accuracy_across_folds}\")\n",
    "# ---- Prediction on Test Set ----\n",
    "test_df = df_test[['id', 'Category']].copy()\n",
    "test_df['id'] = test_df['id'].astype(str)\n",
    "test_df['filename'] = test_df['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_test_category = test_df[test_df['Category'] == category]\n",
    "\n",
    "    for attr in attributes:\n",
    "        model_path = f'best_model_{category}_{attr}.h5'\n",
    "        model = load_model(model_path)\n",
    "        \n",
    "        test_generator = create_data_generator(train=False).flow_from_dataframe(\n",
    "            dataframe=df_test_category, directory=test_image_dir, x_col='filename', y_col=None,\n",
    "            target_size=(224, 224), batch_size=32, class_mode=None, shuffle=False)\n",
    "        \n",
    "        predictions = model.predict(test_generator)\n",
    "        \n",
    "        # Decode predictions based on encoding type\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        decoded_predictions = label_encoders[category][attr].inverse_transform(predicted_classes)\n",
    "\n",
    "        all_predictions.append(pd.DataFrame({\n",
    "            'filename': df_test_category['filename'], f'{attr}_pred': decoded_predictions\n",
    "        }))\n",
    "\n",
    "# Merge all attribute predictions and save\n",
    "submission_df = pd.concat(all_predictions, axis=1)\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file created: submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attr_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    #'Men Tshirts': ['attr_1','attr_2', 'attr_3', 'attr_4','attr_5'],\n",
    "    #'Sarees': ['attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10'],\n",
    "    'Kurtis': [ 'attr_4'],\n",
    "    #'Women Tshirts': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8'],\n",
    "    #'Women Tops and Tunics': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10']\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator(train=True):\n",
    "    if train:\n",
    "        return ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,\n",
    "                                  shear_range=0.2, zoom_range=0.4, horizontal_flip=True, fill_mode='nearest')\n",
    "    else:\n",
    "        return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define model-building function\n",
    "def build_model(num_classes):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dropout(0.4),\n",
    "    ])\n",
    "    for layer in base_model.layers[-20:]:\n",
    "        layer.trainable = True\n",
    "    for layer in base_model.layers[:-20]:  # Adjust this value to freeze more or fewer layers\n",
    "        layer.trainable = False\n",
    "    # Add L2 regularization with a lambda value (you can adjust the value)\n",
    "    l2_lambda = 0.001  # Example value; you can adjust it based on your experiments\n",
    "    \n",
    "    if num_classes == 2:  # Binary classification\n",
    "        model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.00001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    else:  # Multi-class classification with sparse labels\n",
    "        model.add(Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.00001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "# Training process for each category and attribute\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        num_classes = df_attr[attr].nunique()\n",
    "        binary = (num_classes == 2)\n",
    "        \n",
    "        # Use label encoding for all attributes\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "        if binary:\n",
    "            df_attr['label'] = df_attr['label'].astype(str)\n",
    "\n",
    "        # Calculate class weights\n",
    "       # class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(df_attr['label']), y=df_attr['label'])\n",
    "        #class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_accuracy_across_folds = 0\n",
    "        best_model_path = f'best_model_{category}_{attr}.h5'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        best_fold_accuracy = 0\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Create data generators\n",
    "            train_generator = create_data_generator(train=True).flow_from_dataframe(\n",
    "                dataframe=train_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw')\n",
    "\n",
    "            val_generator = create_data_generator(train=False).flow_from_dataframe(\n",
    "                dataframe=val_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw', shuffle=False)\n",
    "\n",
    "            # Build the model\n",
    "            model = build_model(num_classes=num_classes)\n",
    "\n",
    "            # Set up variables to track the best model in this fold\n",
    "            \n",
    "            best_fold_model_path = f'best_model_{category}_{attr}_fold{fold+1}.h5'\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-7, verbose=1)\n",
    "            \n",
    "\n",
    "            # Train for multiple epochs and save the model for the best epoch in this fold\n",
    "            for epoch in range(10):\n",
    "                print(f\"Epoch {epoch+1}/{10}\")\n",
    "                history = model.fit(\n",
    "                    train_generator, validation_data=val_generator, epochs=1, verbose=1,callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "                # Get the validation accuracy of the current epoch\n",
    "                val_accuracy = history.history['val_accuracy'][0]\n",
    "\n",
    "                # If the current epoch's accuracy is the best, save the model for this epoch\n",
    "                if val_accuracy > best_fold_accuracy:\n",
    "                    best_fold_accuracy = val_accuracy\n",
    "                    model.save(best_fold_model_path)\n",
    "                    print(f\"New best model saved for fold {fold+1} at epoch {epoch+1} with accuracy: {val_accuracy}\")\n",
    "\n",
    "            # After the fold, check if this fold's best model is better than previous folds\n",
    "            if best_fold_accuracy > best_accuracy_across_folds:\n",
    "                best_accuracy_across_folds = best_fold_accuracy\n",
    "                best_model_path = best_fold_model_path\n",
    "                print(f\"Best model across all folds so far for {category} - {attr} saved with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "        print(f\"Overall best model for {category} - {attr} saved at: {best_model_path} with accuracy: {best_accuracy_across_folds}\")\n",
    "# ---- Prediction on Test Set ----\n",
    "test_df = df_test[['id', 'Category']].copy()\n",
    "test_df['id'] = test_df['id'].astype(str)\n",
    "test_df['filename'] = test_df['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_test_category = test_df[test_df['Category'] == category]\n",
    "\n",
    "    for attr in attributes:\n",
    "        model_path = f'best_model_{category}_{attr}.h5'\n",
    "        model = load_model(model_path)\n",
    "        \n",
    "        test_generator = create_data_generator(train=False).flow_from_dataframe(\n",
    "            dataframe=df_test_category, directory=test_image_dir, x_col='filename', y_col=None,\n",
    "            target_size=(224, 224), batch_size=32, class_mode=None, shuffle=False)\n",
    "        \n",
    "        predictions = model.predict(test_generator)\n",
    "        \n",
    "        # Decode predictions based on encoding type\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        decoded_predictions = label_encoders[category][attr].inverse_transform(predicted_classes)\n",
    "\n",
    "        all_predictions.append(pd.DataFrame({\n",
    "            'filename': df_test_category['filename'], f'{attr}_pred': decoded_predictions\n",
    "        }))\n",
    "\n",
    "# Merge all attribute predictions and save\n",
    "submission_df = pd.concat(all_predictions, axis=1)\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file created: submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attr_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    #'Men Tshirts': ['attr_1','attr_2', 'attr_3', 'attr_4','attr_5'],\n",
    "    #'Sarees': ['attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10'],\n",
    "    'Kurtis': [ 'attr_5'],\n",
    "    #'Women Tshirts': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8'],\n",
    "    #'Women Tops and Tunics': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10']\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator(train=True):\n",
    "    if train:\n",
    "        return ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,\n",
    "                                  shear_range=0.2, zoom_range=0.4, horizontal_flip=True, fill_mode='nearest')\n",
    "    else:\n",
    "        return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define model-building function\n",
    "def build_model(num_classes):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dropout(0.4),\n",
    "    ])\n",
    "    for layer in base_model.layers[-20:]:\n",
    "        layer.trainable = True\n",
    "    for layer in base_model.layers[:-20]:  # Adjust this value to freeze more or fewer layers\n",
    "        layer.trainable = False\n",
    "    # Add L2 regularization with a lambda value (you can adjust the value)\n",
    "    l2_lambda = 0.001  # Example value; you can adjust it based on your experiments\n",
    "    \n",
    "    if num_classes == 2:  # Binary classification\n",
    "        model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.00001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    else:  # Multi-class classification with sparse labels\n",
    "        model.add(Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.00001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "# Training process for each category and attribute\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        num_classes = df_attr[attr].nunique()\n",
    "        binary = (num_classes == 2)\n",
    "        \n",
    "        # Use label encoding for all attributes\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "        if binary:\n",
    "            df_attr['label'] = df_attr['label'].astype(str)\n",
    "\n",
    "        # Calculate class weights\n",
    "       # class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(df_attr['label']), y=df_attr['label'])\n",
    "        #class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_accuracy_across_folds = 0\n",
    "        best_model_path = f'best_model_{category}_{attr}.h5'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        best_fold_accuracy = 0\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Create data generators\n",
    "            train_generator = create_data_generator(train=True).flow_from_dataframe(\n",
    "                dataframe=train_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw')\n",
    "\n",
    "            val_generator = create_data_generator(train=False).flow_from_dataframe(\n",
    "                dataframe=val_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw', shuffle=False)\n",
    "\n",
    "            # Build the model\n",
    "            model = build_model(num_classes=num_classes)\n",
    "\n",
    "            # Set up variables to track the best model in this fold\n",
    "            \n",
    "            best_fold_model_path = f'best_model_{category}_{attr}_fold{fold+1}.h5'\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-7, verbose=1)\n",
    "            \n",
    "\n",
    "            # Train for multiple epochs and save the model for the best epoch in this fold\n",
    "            for epoch in range(10):\n",
    "                print(f\"Epoch {epoch+1}/{10}\")\n",
    "                history = model.fit(\n",
    "                    train_generator, validation_data=val_generator, epochs=1, verbose=1,callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "                # Get the validation accuracy of the current epoch\n",
    "                val_accuracy = history.history['val_accuracy'][0]\n",
    "\n",
    "                # If the current epoch's accuracy is the best, save the model for this epoch\n",
    "                if val_accuracy > best_fold_accuracy:\n",
    "                    best_fold_accuracy = val_accuracy\n",
    "                    model.save(best_fold_model_path)\n",
    "                    print(f\"New best model saved for fold {fold+1} at epoch {epoch+1} with accuracy: {val_accuracy}\")\n",
    "\n",
    "            # After the fold, check if this fold's best model is better than previous folds\n",
    "            if best_fold_accuracy > best_accuracy_across_folds:\n",
    "                best_accuracy_across_folds = best_fold_accuracy\n",
    "                best_model_path = best_fold_model_path\n",
    "                print(f\"Best model across all folds so far for {category} - {attr} saved with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "        print(f\"Overall best model for {category} - {attr} saved at: {best_model_path} with accuracy: {best_accuracy_across_folds}\")\n",
    "# ---- Prediction on Test Set ----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attr_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    #'Men Tshirts': ['attr_1','attr_2', 'attr_3', 'attr_4','attr_5'],\n",
    "    #'Sarees': ['attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10'],\n",
    "    'Kurtis': [ 'attr_6'],\n",
    "    #'Women Tshirts': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8'],\n",
    "    #'Women Tops and Tunics': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10']\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator(train=True):\n",
    "    if train:\n",
    "        return ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,\n",
    "                                  shear_range=0.2, zoom_range=0.4, horizontal_flip=True, fill_mode='nearest')\n",
    "    else:\n",
    "        return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define model-building function\n",
    "def build_model(num_classes):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "    ])\n",
    "    for layer in base_model.layers[:]:\n",
    "        layer.trainable = True\n",
    "  #  for layer in base_model.layers[:-20]:  # Adjust this value to freeze more or fewer layers\n",
    "  #      layer.trainable = False\n",
    "    # Add L2 regularization with a lambda value (you can adjust the value)\n",
    "    l2_lambda = 0.001  # Example value; you can adjust it based on your experiments\n",
    "    \n",
    "    if num_classes == 2:  # Binary classification\n",
    "        model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.00001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    else:  # Multi-class classification with sparse labels\n",
    "        model.add(Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.00001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "# Training process for each category and attribute\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        num_classes = df_attr[attr].nunique()\n",
    "        binary = (num_classes == 2)\n",
    "        \n",
    "        # Use label encoding for all attributes\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "        if binary:\n",
    "            df_attr['label'] = df_attr['label'].astype(str)\n",
    "\n",
    "        # Calculate class weights\n",
    "       # class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(df_attr['label']), y=df_attr['label'])\n",
    "        #class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_accuracy_across_folds = 0\n",
    "        best_model_path = f'best_model_{category}_{attr}.h5'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        best_fold_accuracy = 0\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Create data generators\n",
    "            train_generator = create_data_generator(train=True).flow_from_dataframe(\n",
    "                dataframe=train_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw')\n",
    "\n",
    "            val_generator = create_data_generator(train=False).flow_from_dataframe(\n",
    "                dataframe=val_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw', shuffle=False)\n",
    "\n",
    "            # Build the model\n",
    "            model = build_model(num_classes=num_classes)\n",
    "\n",
    "            # Set up variables to track the best model in this fold\n",
    "            \n",
    "            best_fold_model_path = f'best_model_{category}_{attr}_fold{fold+1}.h5'\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-7, verbose=1)\n",
    "            \n",
    "\n",
    "            # Train for multiple epochs and save the model for the best epoch in this fold\n",
    "            for epoch in range(10):\n",
    "                print(f\"Epoch {epoch+1}/{10}\")\n",
    "                history = model.fit(\n",
    "                    train_generator, validation_data=val_generator, epochs=1, verbose=1,callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "                # Get the validation accuracy of the current epoch\n",
    "                val_accuracy = history.history['val_accuracy'][0]\n",
    "\n",
    "                # If the current epoch's accuracy is the best, save the model for this epoch\n",
    "                if val_accuracy > best_fold_accuracy:\n",
    "                    best_fold_accuracy = val_accuracy\n",
    "                    model.save(best_fold_model_path)\n",
    "                    print(f\"New best model saved for fold {fold+1} at epoch {epoch+1} with accuracy: {val_accuracy}\")\n",
    "\n",
    "            # After the fold, check if this fold's best model is better than previous folds\n",
    "            if best_fold_accuracy > best_accuracy_across_folds:\n",
    "                best_accuracy_across_folds = best_fold_accuracy\n",
    "                best_model_path = best_fold_model_path\n",
    "                print(f\"Best model across all folds so far for {category} - {attr} saved with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "        print(f\"Overall best model for {category} - {attr} saved at: {best_model_path} with accuracy: {best_accuracy_across_folds}\")\n",
    "# ---- Prediction on Test Set ----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attr_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    #'Men Tshirts': ['attr_1','attr_2', 'attr_3', 'attr_4','attr_5'],\n",
    "    #'Sarees': ['attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10'],\n",
    "    'Kurtis': [ 'attr_7'],\n",
    "    #'Women Tshirts': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8'],\n",
    "    #'Women Tops and Tunics': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10']\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator(train=True):\n",
    "    if train:\n",
    "        return ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,\n",
    "                                  shear_range=0.2, zoom_range=0.4, horizontal_flip=True, fill_mode='nearest')\n",
    "    else:\n",
    "        return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define model-building function\n",
    "def build_model(num_classes):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "    ])\n",
    "    for layer in base_model.layers[-20:]:\n",
    "        layer.trainable = True\n",
    "    for layer in base_model.layers[:-20]:  # Adjust this value to freeze more or fewer layers\n",
    "        layer.trainable = False\n",
    "    # Add L2 regularization with a lambda value (you can adjust the value)\n",
    "    l2_lambda = 0.001  # Example value; you can adjust it based on your experiments\n",
    "    \n",
    "    if num_classes == 2:  # Binary classification\n",
    "        model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.00001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    else:  # Multi-class classification with sparse labels\n",
    "        model.add(Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.00001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "# Training process for each category and attribute\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        num_classes = df_attr[attr].nunique()\n",
    "        binary = (num_classes == 2)\n",
    "        \n",
    "        # Use label encoding for all attributes\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "        if binary:\n",
    "            df_attr['label'] = df_attr['label'].astype(str)\n",
    "\n",
    "        # Calculate class weights\n",
    "       # class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(df_attr['label']), y=df_attr['label'])\n",
    "        #class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_accuracy_across_folds = 0\n",
    "        best_model_path = f'best_model_{category}_{attr}.h5'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        best_fold_accuracy = 0\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Create data generators\n",
    "            train_generator = create_data_generator(train=True).flow_from_dataframe(\n",
    "                dataframe=train_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw')\n",
    "\n",
    "            val_generator = create_data_generator(train=False).flow_from_dataframe(\n",
    "                dataframe=val_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw', shuffle=False)\n",
    "\n",
    "            # Build the model\n",
    "            model = build_model(num_classes=num_classes)\n",
    "\n",
    "            # Set up variables to track the best model in this fold\n",
    "            \n",
    "            best_fold_model_path = f'best_model_{category}_{attr}_fold{fold+1}.h5'\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-7, verbose=1)\n",
    "            \n",
    "\n",
    "            # Train for multiple epochs and save the model for the best epoch in this fold\n",
    "            for epoch in range(10):\n",
    "                print(f\"Epoch {epoch+1}/{10}\")\n",
    "                history = model.fit(\n",
    "                    train_generator, validation_data=val_generator, epochs=1, verbose=1,callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "                # Get the validation accuracy of the current epoch\n",
    "                val_accuracy = history.history['val_accuracy'][0]\n",
    "\n",
    "                # If the current epoch's accuracy is the best, save the model for this epoch\n",
    "                if val_accuracy > best_fold_accuracy:\n",
    "                    best_fold_accuracy = val_accuracy\n",
    "                    model.save(best_fold_model_path)\n",
    "                    print(f\"New best model saved for fold {fold+1} at epoch {epoch+1} with accuracy: {val_accuracy}\")\n",
    "\n",
    "            # After the fold, check if this fold's best model is better than previous folds\n",
    "            if best_fold_accuracy > best_accuracy_across_folds:\n",
    "                best_accuracy_across_folds = best_fold_accuracy\n",
    "                best_model_path = best_fold_model_path\n",
    "                print(f\"Best model across all folds so far for {category} - {attr} saved with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "        print(f\"Overall best model for {category} - {attr} saved at: {best_model_path} with accuracy: {best_accuracy_across_folds}\")\n",
    "# ---- Prediction on Test Set ----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attr_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    #'Men Tshirts': ['attr_1','attr_2', 'attr_3', 'attr_4','attr_5'],\n",
    "    #'Sarees': ['attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10'],\n",
    "    'Kurtis': [ 'attr_8'],\n",
    "    #'Women Tshirts': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8'],\n",
    "    #'Women Tops and Tunics': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10']\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator(train=True):\n",
    "    if train:\n",
    "        return ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,\n",
    "                                  shear_range=0.2, zoom_range=0.4, horizontal_flip=True, fill_mode='nearest')\n",
    "    else:\n",
    "        return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define model-building function\n",
    "def build_model(num_classes):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "    ])\n",
    "    for layer in base_model.layers[-20:]:\n",
    "        layer.trainable = True\n",
    "    for layer in base_model.layers[:-20]:  # Adjust this value to freeze more or fewer layers\n",
    "        layer.trainable = False\n",
    "    # Add L2 regularization with a lambda value (you can adjust the value)\n",
    "    l2_lambda = 0.001  # Example value; you can adjust it based on your experiments\n",
    "    \n",
    "    if num_classes == 2:  # Binary classification\n",
    "        model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.00001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    else:  # Multi-class classification with sparse labels\n",
    "        model.add(Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.00001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "# Training process for each category and attribute\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        num_classes = df_attr[attr].nunique()\n",
    "        binary = (num_classes == 2)\n",
    "        \n",
    "        # Use label encoding for all attributes\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "        if binary:\n",
    "            df_attr['label'] = df_attr['label'].astype(str)\n",
    "\n",
    "        # Calculate class weights\n",
    "       # class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(df_attr['label']), y=df_attr['label'])\n",
    "        #class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_accuracy_across_folds = 0\n",
    "        best_model_path = f'best_model_{category}_{attr}.h5'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        best_fold_accuracy = 0\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Create data generators\n",
    "            train_generator = create_data_generator(train=True).flow_from_dataframe(\n",
    "                dataframe=train_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw')\n",
    "\n",
    "            val_generator = create_data_generator(train=False).flow_from_dataframe(\n",
    "                dataframe=val_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw', shuffle=False)\n",
    "\n",
    "            # Build the model\n",
    "            model = build_model(num_classes=num_classes)\n",
    "\n",
    "            # Set up variables to track the best model in this fold\n",
    "            \n",
    "            best_fold_model_path = f'best_model_{category}_{attr}_fold{fold+1}.h5'\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-7, verbose=1)\n",
    "            \n",
    "\n",
    "            # Train for multiple epochs and save the model for the best epoch in this fold\n",
    "            for epoch in range(10):\n",
    "                print(f\"Epoch {epoch+1}/{10}\")\n",
    "                history = model.fit(\n",
    "                    train_generator, validation_data=val_generator, epochs=1, verbose=1,callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "                # Get the validation accuracy of the current epoch\n",
    "                val_accuracy = history.history['val_accuracy'][0]\n",
    "\n",
    "                # If the current epoch's accuracy is the best, save the model for this epoch\n",
    "                if val_accuracy > best_fold_accuracy:\n",
    "                    best_fold_accuracy = val_accuracy\n",
    "                    model.save(best_fold_model_path)\n",
    "                    print(f\"New best model saved for fold {fold+1} at epoch {epoch+1} with accuracy: {val_accuracy}\")\n",
    "\n",
    "            # After the fold, check if this fold's best model is better than previous folds\n",
    "            if best_fold_accuracy > best_accuracy_across_folds:\n",
    "                best_accuracy_across_folds = best_fold_accuracy\n",
    "                best_model_path = best_fold_model_path\n",
    "                print(f\"Best model across all folds so far for {category} - {attr} saved with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "        print(f\"Overall best model for {category} - {attr} saved at: {best_model_path} with accuracy: {best_accuracy_across_folds}\")\n",
    "# ---- Prediction on Test Set ----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attr_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    #'Men Tshirts': ['attr_1','attr_2', 'attr_3', 'attr_4','attr_5'],\n",
    "    #'Sarees': ['attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10'],\n",
    "    'Kurtis': [ 'attr_9'],\n",
    "    #'Women Tshirts': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8'],\n",
    "    #'Women Tops and Tunics': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10']\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator(train=True):\n",
    "    if train:\n",
    "        return ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,\n",
    "                                  shear_range=0.2, zoom_range=0.4, horizontal_flip=True, fill_mode='nearest')\n",
    "    else:\n",
    "        return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define model-building function\n",
    "def build_model(num_classes):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "    ])\n",
    "    for layer in base_model.layers[-20:]:\n",
    "        layer.trainable = True\n",
    "    for layer in base_model.layers[:-20]:  # Adjust this value to freeze more or fewer layers\n",
    "        layer.trainable = False\n",
    "    # Add L2 regularization with a lambda value (you can adjust the value)\n",
    "    l2_lambda = 0.001  # Example value; you can adjust it based on your experiments\n",
    "    \n",
    "    if num_classes == 2:  # Binary classification\n",
    "        model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.00001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    else:  # Multi-class classification with sparse labels\n",
    "        model.add(Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.00001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "# Training process for each category and attribute\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        num_classes = df_attr[attr].nunique()\n",
    "        binary = (num_classes == 2)\n",
    "        \n",
    "        # Use label encoding for all attributes\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "        if binary:\n",
    "            df_attr['label'] = df_attr['label'].astype(str)\n",
    "\n",
    "        # Calculate class weights\n",
    "       # class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(df_attr['label']), y=df_attr['label'])\n",
    "        #class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_accuracy_across_folds = 0\n",
    "        best_model_path = f'best_model_{category}_{attr}.h5'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        best_fold_accuracy = 0\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Create data generators\n",
    "            train_generator = create_data_generator(train=True).flow_from_dataframe(\n",
    "                dataframe=train_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw')\n",
    "\n",
    "            val_generator = create_data_generator(train=False).flow_from_dataframe(\n",
    "                dataframe=val_fold, directory=train_image_dir, x_col='filename', y_col='label',\n",
    "                target_size=(224, 224), batch_size=32, class_mode='binary' if binary else 'raw', shuffle=False)\n",
    "\n",
    "            # Build the model\n",
    "            model = build_model(num_classes=num_classes)\n",
    "\n",
    "            # Set up variables to track the best model in this fold\n",
    "            \n",
    "            best_fold_model_path = f'best_model_{category}_{attr}_fold{fold+1}.h5'\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-7, verbose=1)\n",
    "            \n",
    "\n",
    "            # Train for multiple epochs and save the model for the best epoch in this fold\n",
    "            for epoch in range(10):\n",
    "                print(f\"Epoch {epoch+1}/{10}\")\n",
    "                history = model.fit(\n",
    "                    train_generator, validation_data=val_generator, epochs=1, verbose=1,callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "                # Get the validation accuracy of the current epoch\n",
    "                val_accuracy = history.history['val_accuracy'][0]\n",
    "\n",
    "                # If the current epoch's accuracy is the best, save the model for this epoch\n",
    "                if val_accuracy > best_fold_accuracy:\n",
    "                    best_fold_accuracy = val_accuracy\n",
    "                    model.save(best_fold_model_path)\n",
    "                    print(f\"New best model saved for fold {fold+1} at epoch {epoch+1} with accuracy: {val_accuracy}\")\n",
    "\n",
    "            # After the fold, check if this fold's best model is better than previous folds\n",
    "            if best_fold_accuracy > best_accuracy_across_folds:\n",
    "                best_accuracy_across_folds = best_fold_accuracy\n",
    "                best_model_path = best_fold_model_path\n",
    "                print(f\"Best model across all folds so far for {category} - {attr} saved with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "        print(f\"Overall best model for {category} - {attr} saved at: {best_model_path} with accuracy: {best_accuracy_across_folds}\")\n",
    "# ---- Prediction on Test Set ----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resnet_50 for extracting features and catboost classifier for predicting attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attr_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from catboost import CatBoostClassifier\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    'Kurtis': ['attr_1'],\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator with augmentation\n",
    "def create_data_generator():\n",
    "    return ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=35,\n",
    "        width_shift_range=0.28,\n",
    "        height_shift_range=0.29,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "# Define feature extraction function using VGG16\n",
    "def extract_features(image_files, image_dir, model, target_size=(224, 224), batch_size=32):\n",
    "    data_gen = create_data_generator()\n",
    "    generator = data_gen.flow_from_dataframe(\n",
    "        dataframe=image_files, directory=image_dir, x_col='filename', y_col=None,\n",
    "        target_size=target_size, batch_size=batch_size, class_mode=None, shuffle=False\n",
    "    )\n",
    "    features = model.predict(generator, verbose=1)\n",
    "    return features\n",
    "\n",
    "# Load pre-trained VGG16 without the fully connected layers, for feature extraction\n",
    "feature_extractor = VGG16(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n",
    "\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        num_classes = df_attr[attr].nunique()\n",
    "        \n",
    "        # Use label encoding for all attributes\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_accuracy_across_folds = 0\n",
    "        best_model_path = f'best_catboost_model_{category}_{attr}.cbm'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Extract features for train and validation sets\n",
    "            train_features = extract_features(train_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            val_features = extract_features(val_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            \n",
    "            # Train CatBoost model with GPU support and early stopping\n",
    "            catboost_model = CatBoostClassifier(\n",
    "                iterations=3000, learning_rate=0.05, depth=10, task_type=\"GPU\", random_seed=42,\n",
    "                early_stopping_rounds=100, verbose=100\n",
    "            )\n",
    "            catboost_model.fit(train_features, train_fold['label'], eval_set=(val_features, val_fold['label']), verbose=100)\n",
    "            \n",
    "            # Validate the model\n",
    "            val_preds = catboost_model.predict(val_features)\n",
    "            val_accuracy = accuracy_score(val_fold['label'], val_preds)\n",
    "            val_f1 = f1_score(val_fold['label'], val_preds, average='weighted')\n",
    "            print(f\"Validation accuracy for fold {fold+1}: {val_accuracy}\")\n",
    "            print(f\"Validation F1 score for fold {fold+1}: {val_f1}\")\n",
    "\n",
    "            # Check if this fold's accuracy is the best so far\n",
    "            if val_accuracy > best_accuracy_across_folds:\n",
    "                best_accuracy_across_folds = val_accuracy\n",
    "                best_f1_score_across_folds = val_f1\n",
    "                # Save the model here if you want, e.g., using joblib or pickle\n",
    "                catboost_model.save_model(best_model_path)\n",
    "                print(f\"New best model for {category} - {attr} with accuracy: {best_accuracy_across_folds} and F1: {best_f1_score_across_folds}\")\n",
    "\n",
    "        print(f\"Best model across all folds for {category} - {attr} with accuracy: {best_accuracy_across_folds} and F1 score: {best_f1_score_across_folds}\")\n",
    "\n",
    "# ---- Prediction on Test Set ----\n",
    "all_predictions = []\n",
    "\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_test_category = df_test[df_test['Category'] == category]\n",
    "    df_test_category['id'] = df_test_category['id'].astype(str)\n",
    "    df_test_category['filename'] = df_test_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "    \n",
    "    for attr in attributes:\n",
    "        print(f\"Predicting on test set for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Load the best model (if saved)\n",
    "        catboost_model = CatBoostClassifier()\n",
    "        catboost_model.load_model(best_model_path)\n",
    "        \n",
    "        # Extract features for the test set\n",
    "        test_features = extract_features(df_test_category[['filename']], test_image_dir, feature_extractor)\n",
    "        \n",
    "        # Make predictions\n",
    "        test_preds = catboost_model.predict(test_features)\n",
    "        \n",
    "        # Convert labels back to original categories\n",
    "        test_preds_decoded = label_encoders[category][attr].inverse_transform(test_preds)\n",
    "        \n",
    "        # Ensure alignment between predictions and the test dataframe\n",
    "        df_test_category[f'predicted_{attr}'] = test_preds_decoded\n",
    "        \n",
    "        # Append the predictions to the main dataframe\n",
    "        all_predictions.append(df_test_category[['id', f'predicted_{attr}']])\n",
    "\n",
    "# Concatenate all the predictions and save to a CSV\n",
    "df_predictions = pd.concat(all_predictions)\n",
    "df_predictions.to_csv('Kurtis_attr_1.csv', index=False)\n",
    "\n",
    "print(\"Test predictions saved to 'Kurtis_attr_1.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attr_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from catboost import CatBoostClassifier\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    'Kurtis': ['attr_5'],\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator():\n",
    "    return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define feature extraction function using ResNet50\n",
    "def extract_features(image_files, image_dir, model, target_size=(224, 224), batch_size=32):\n",
    "    data_gen = create_data_generator()\n",
    "    generator = data_gen.flow_from_dataframe(\n",
    "        dataframe=image_files, directory=image_dir, x_col='filename', y_col=None,\n",
    "        target_size=target_size, batch_size=batch_size, class_mode=None, shuffle=False\n",
    "    )\n",
    "    features = model.predict(generator, verbose=1)\n",
    "    return features\n",
    "\n",
    "# Load pre-trained ResNet50 without the fully connected layers, for feature extraction\n",
    "feature_extractor = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n",
    "\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        num_classes = df_attr[attr].nunique()\n",
    "        \n",
    "        # Use label encoding for all attributes\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_accuracy_across_folds = 0\n",
    "        best_model_path = f'best_catboost_model_{category}_{attr}.cbm'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Extract features for train and validation sets\n",
    "            train_features = extract_features(train_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            val_features = extract_features(val_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            \n",
    "            # Train CatBoost model with GPU support\n",
    "            catboost_model = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, task_type=\"GPU\", random_seed=42, verbose=100)\n",
    "            catboost_model.fit(train_features, train_fold['label'])\n",
    "            \n",
    "            # Validate the model\n",
    "            val_preds = catboost_model.predict(val_features)\n",
    "            val_accuracy = accuracy_score(val_fold['label'], val_preds)\n",
    "            print(f\"Validation accuracy for fold {fold+1}: {val_accuracy}\")\n",
    "\n",
    "            # Check if this fold's accuracy is the best so far\n",
    "            if val_accuracy > best_accuracy_across_folds:\n",
    "                best_accuracy_across_folds = val_accuracy\n",
    "                # Save the model here if you want, e.g., using joblib or pickle\n",
    "                catboost_model.save_model(best_model_path)\n",
    "                print(f\"New best model for {category} - {attr} with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "        print(f\"Best model across all folds for {category} - {attr} with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "# ---- Prediction on Test Set ----\n",
    "all_predictions = []\n",
    "\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_test_category = df_test[df_test['Category'] == category]\n",
    "    df_test_category['id'] = df_test_category['id'].astype(str)\n",
    "    df_test_category['filename'] = df_test_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "    \n",
    "    for attr in attributes:\n",
    "        print(f\"Predicting on test set for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Load the best model (if saved)\n",
    "        catboost_model = CatBoostClassifier()\n",
    "        catboost_model.load_model(best_model_path)\n",
    "        \n",
    "        # Extract features for the test set\n",
    "        test_features = extract_features(df_test_category[['filename']], test_image_dir, feature_extractor)\n",
    "        \n",
    "        # Make predictions\n",
    "        test_preds = catboost_model.predict(test_features)\n",
    "        \n",
    "        # Convert labels back to original categories\n",
    "        test_preds_decoded = label_encoders[category][attr].inverse_transform(test_preds)\n",
    "        \n",
    "        # Ensure alignment between predictions and the test dataframe\n",
    "        df_test_category[f'predicted_{attr}'] = test_preds_decoded\n",
    "        \n",
    "        # Append the predictions to the main dataframe\n",
    "        all_predictions.append(df_test_category[['id', f'predicted_{attr}']])\n",
    "\n",
    "# Concatenate all the predictions and save to a CSV\n",
    "df_predictions = pd.concat(all_predictions)\n",
    "df_predictions.to_csv('Kurtis_attr_5.csv', index=False)\n",
    "\n",
    "print(\"Test predictions saved to 'Kurtis_attr_5.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attr_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from catboost import CatBoostClassifier\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    'Kurtis': ['attr_6'],\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator():\n",
    "    return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define feature extraction function using ResNet50\n",
    "def extract_features(image_files, image_dir, model, target_size=(224, 224), batch_size=32):\n",
    "    data_gen = create_data_generator()\n",
    "    generator = data_gen.flow_from_dataframe(\n",
    "        dataframe=image_files, directory=image_dir, x_col='filename', y_col=None,\n",
    "        target_size=target_size, batch_size=batch_size, class_mode=None, shuffle=False\n",
    "    )\n",
    "    features = model.predict(generator, verbose=1)\n",
    "    return features\n",
    "\n",
    "# Load pre-trained ResNet50 without the fully connected layers, for feature extraction\n",
    "feature_extractor = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n",
    "\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        num_classes = df_attr[attr].nunique()\n",
    "        \n",
    "        # Use label encoding for all attributes\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_accuracy_across_folds = 0\n",
    "        best_model_path = f'best_catboost_model_{category}_{attr}.cbm'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Extract features for train and validation sets\n",
    "            train_features = extract_features(train_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            val_features = extract_features(val_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            \n",
    "            # Train CatBoost model with GPU support\n",
    "            catboost_model = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, task_type=\"GPU\", random_seed=42, verbose=100)\n",
    "            catboost_model.fit(train_features, train_fold['label'])\n",
    "            \n",
    "            # Validate the model\n",
    "            val_preds = catboost_model.predict(val_features)\n",
    "            val_accuracy = accuracy_score(val_fold['label'], val_preds)\n",
    "            print(f\"Validation accuracy for fold {fold+1}: {val_accuracy}\")\n",
    "\n",
    "            # Check if this fold's accuracy is the best so far\n",
    "            if val_accuracy > best_accuracy_across_folds:\n",
    "                best_accuracy_across_folds = val_accuracy\n",
    "                # Save the model here if you want, e.g., using joblib or pickle\n",
    "                catboost_model.save_model(best_model_path)\n",
    "                print(f\"New best model for {category} - {attr} with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "        print(f\"Best model across all folds for {category} - {attr} with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "# ---- Prediction on Test Set ----\n",
    "all_predictions = []\n",
    "\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_test_category = df_test[df_test['Category'] == category]\n",
    "    df_test_category['id'] = df_test_category['id'].astype(str)\n",
    "    df_test_category['filename'] = df_test_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "    \n",
    "    for attr in attributes:\n",
    "        print(f\"Predicting on test set for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Load the best model (if saved)\n",
    "        catboost_model = CatBoostClassifier()\n",
    "        catboost_model.load_model(best_model_path)\n",
    "        \n",
    "        # Extract features for the test set\n",
    "        test_features = extract_features(df_test_category[['filename']], test_image_dir, feature_extractor)\n",
    "        \n",
    "        # Make predictions\n",
    "        test_preds = catboost_model.predict(test_features)\n",
    "        \n",
    "        # Convert labels back to original categories\n",
    "        test_preds_decoded = label_encoders[category][attr].inverse_transform(test_preds)\n",
    "        \n",
    "        # Ensure alignment between predictions and the test dataframe\n",
    "        df_test_category[f'predicted_{attr}'] = test_preds_decoded\n",
    "        \n",
    "        # Append the predictions to the main dataframe\n",
    "        all_predictions.append(df_test_category[['id', f'predicted_{attr}']])\n",
    "\n",
    "# Concatenate all the predictions and save to a CSV\n",
    "df_predictions = pd.concat(all_predictions)\n",
    "df_predictions.to_csv('Kurtis_attr_6.csv', index=False)\n",
    "\n",
    "print(\"Test predictions saved to 'Kurtis_attr_6.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attr_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from catboost import CatBoostClassifier\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    'Kurtis': ['attr_7'],\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator():\n",
    "    return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define feature extraction function using ResNet50\n",
    "def extract_features(image_files, image_dir, model, target_size=(224, 224), batch_size=32):\n",
    "    data_gen = create_data_generator()\n",
    "    generator = data_gen.flow_from_dataframe(\n",
    "        dataframe=image_files, directory=image_dir, x_col='filename', y_col=None,\n",
    "        target_size=target_size, batch_size=batch_size, class_mode=None, shuffle=False\n",
    "    )\n",
    "    features = model.predict(generator, verbose=1)\n",
    "    return features\n",
    "\n",
    "# Load pre-trained ResNet50 without the fully connected layers, for feature extraction\n",
    "feature_extractor = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n",
    "\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        num_classes = df_attr[attr].nunique()\n",
    "        \n",
    "        # Use label encoding for all attributes\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_accuracy_across_folds = 0\n",
    "        best_model_path = f'best_catboost_model_{category}_{attr}.cbm'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Extract features for train and validation sets\n",
    "            train_features = extract_features(train_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            val_features = extract_features(val_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            \n",
    "            # Train CatBoost model with GPU support\n",
    "            catboost_model = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, task_type=\"GPU\", random_seed=42, verbose=100)\n",
    "            catboost_model.fit(train_features, train_fold['label'])\n",
    "            \n",
    "            # Validate the model\n",
    "            val_preds = catboost_model.predict(val_features)\n",
    "            val_accuracy = accuracy_score(val_fold['label'], val_preds)\n",
    "            print(f\"Validation accuracy for fold {fold+1}: {val_accuracy}\")\n",
    "\n",
    "            # Check if this fold's accuracy is the best so far\n",
    "            if val_accuracy > best_accuracy_across_folds:\n",
    "                best_accuracy_across_folds = val_accuracy\n",
    "                # Save the model here if you want, e.g., using joblib or pickle\n",
    "                catboost_model.save_model(best_model_path)\n",
    "                print(f\"New best model for {category} - {attr} with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "        print(f\"Best model across all folds for {category} - {attr} with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "# ---- Prediction on Test Set ----\n",
    "all_predictions = []\n",
    "\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_test_category = df_test[df_test['Category'] == category]\n",
    "    df_test_category['id'] = df_test_category['id'].astype(str)\n",
    "    df_test_category['filename'] = df_test_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "    \n",
    "    for attr in attributes:\n",
    "        print(f\"Predicting on test set for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Load the best model (if saved)\n",
    "        catboost_model = CatBoostClassifier()\n",
    "        catboost_model.load_model(best_model_path)\n",
    "        \n",
    "        # Extract features for the test set\n",
    "        test_features = extract_features(df_test_category[['filename']], test_image_dir, feature_extractor)\n",
    "        \n",
    "        # Make predictions\n",
    "        test_preds = catboost_model.predict(test_features)\n",
    "        \n",
    "        # Convert labels back to original categories\n",
    "        test_preds_decoded = label_encoders[category][attr].inverse_transform(test_preds)\n",
    "        \n",
    "        # Ensure alignment between predictions and the test dataframe\n",
    "        df_test_category[f'predicted_{attr}'] = test_preds_decoded\n",
    "        \n",
    "        # Append the predictions to the main dataframe\n",
    "        all_predictions.append(df_test_category[['id', f'predicted_{attr}']])\n",
    "\n",
    "# Concatenate all the predictions and save to a CSV\n",
    "df_predictions = pd.concat(all_predictions)\n",
    "df_predictions.to_csv('Kurtis_attr_7.csv', index=False)\n",
    "\n",
    "print(\"Test predictions saved to 'Kurtis_attr_7.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attr_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from catboost import CatBoostClassifier\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    'Kurtis': ['attr_8'],\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator():\n",
    "    return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define feature extraction function using ResNet50\n",
    "def extract_features(image_files, image_dir, model, target_size=(224, 224), batch_size=32):\n",
    "    data_gen = create_data_generator()\n",
    "    generator = data_gen.flow_from_dataframe(\n",
    "        dataframe=image_files, directory=image_dir, x_col='filename', y_col=None,\n",
    "        target_size=target_size, batch_size=batch_size, class_mode=None, shuffle=False\n",
    "    )\n",
    "    features = model.predict(generator, verbose=1)\n",
    "    return features\n",
    "\n",
    "# Load pre-trained ResNet50 without the fully connected layers, for feature extraction\n",
    "feature_extractor = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n",
    "\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        num_classes = df_attr[attr].nunique()\n",
    "        \n",
    "        # Use label encoding for all attributes\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_accuracy_across_folds = 0\n",
    "        best_model_path = f'best_catboost_model_{category}_{attr}.cbm'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Extract features for train and validation sets\n",
    "            train_features = extract_features(train_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            val_features = extract_features(val_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            \n",
    "            # Train CatBoost model with GPU support\n",
    "            catboost_model = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, task_type=\"GPU\", random_seed=42, verbose=100)\n",
    "            catboost_model.fit(train_features, train_fold['label'])\n",
    "            \n",
    "            # Validate the model\n",
    "            val_preds = catboost_model.predict(val_features)\n",
    "            val_accuracy = accuracy_score(val_fold['label'], val_preds)\n",
    "            print(f\"Validation accuracy for fold {fold+1}: {val_accuracy}\")\n",
    "\n",
    "            # Check if this fold's accuracy is the best so far\n",
    "            if val_accuracy > best_accuracy_across_folds:\n",
    "                best_accuracy_across_folds = val_accuracy\n",
    "                # Save the model here if you want, e.g., using joblib or pickle\n",
    "                catboost_model.save_model(best_model_path)\n",
    "                print(f\"New best model for {category} - {attr} with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "        print(f\"Best model across all folds for {category} - {attr} with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "# ---- Prediction on Test Set ----\n",
    "all_predictions = []\n",
    "\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_test_category = df_test[df_test['Category'] == category]\n",
    "    df_test_category['id'] = df_test_category['id'].astype(str)\n",
    "    df_test_category['filename'] = df_test_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "    \n",
    "    for attr in attributes:\n",
    "        print(f\"Predicting on test set for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Load the best model (if saved)\n",
    "        catboost_model = CatBoostClassifier()\n",
    "        catboost_model.load_model(best_model_path)\n",
    "        \n",
    "        # Extract features for the test set\n",
    "        test_features = extract_features(df_test_category[['filename']], test_image_dir, feature_extractor)\n",
    "        \n",
    "        # Make predictions\n",
    "        test_preds = catboost_model.predict(test_features)\n",
    "        \n",
    "        # Convert labels back to original categories\n",
    "        test_preds_decoded = label_encoders[category][attr].inverse_transform(test_preds)\n",
    "        \n",
    "        # Ensure alignment between predictions and the test dataframe\n",
    "        df_test_category[f'predicted_{attr}'] = test_preds_decoded\n",
    "        \n",
    "        # Append the predictions to the main dataframe\n",
    "        all_predictions.append(df_test_category[['id', f'predicted_{attr}']])\n",
    "\n",
    "# Concatenate all the predictions and save to a CSV\n",
    "df_predictions = pd.concat(all_predictions)\n",
    "df_predictions.to_csv('Kurtis_attr_8.csv', index=False)\n",
    "\n",
    "print(\"Test predictions saved to 'Kurtis_attr_8.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attr_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from catboost import CatBoostClassifier\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Load train and test sets\n",
    "df_train = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/meesho/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/meesho/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/meesho/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    'Kurtis': ['attr_9'],\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a data generator\n",
    "def create_data_generator():\n",
    "    return ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define feature extraction function using ResNet50\n",
    "def extract_features(image_files, image_dir, model, target_size=(224, 224), batch_size=32):\n",
    "    data_gen = create_data_generator()\n",
    "    generator = data_gen.flow_from_dataframe(\n",
    "        dataframe=image_files, directory=image_dir, x_col='filename', y_col=None,\n",
    "        target_size=target_size, batch_size=batch_size, class_mode=None, shuffle=False\n",
    "    )\n",
    "    features = model.predict(generator, verbose=1)\n",
    "    return features\n",
    "\n",
    "# Load pre-trained ResNet50 without the fully connected layers, for feature extraction\n",
    "feature_extractor = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n",
    "\n",
    "# Training process for each category and attribute\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    # Iterate over each attribute\n",
    "    for attr in attributes:\n",
    "        print(f\"Training for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Drop columns not relevant to the current attribute and handle NaNs\n",
    "        df_attr = df_category[['filename', attr]].dropna()\n",
    "        num_classes = df_attr[attr].nunique()\n",
    "        \n",
    "        # Use label encoding for all attributes\n",
    "        le = LabelEncoder()\n",
    "        df_attr['label'] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "\n",
    "        # Set up K-Fold Cross-Validation\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        best_accuracy_across_folds = 0\n",
    "        best_model_path = f'best_catboost_model_{category}_{attr}.cbm'\n",
    "        \n",
    "        # Loop for each fold\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "            print(f\"Training fold {fold+1} for {category} - {attr}\")\n",
    "            train_fold = df_attr.iloc[train_idx]\n",
    "            val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "            # Extract features for train and validation sets\n",
    "            train_features = extract_features(train_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            val_features = extract_features(val_fold[['filename']], train_image_dir, feature_extractor)\n",
    "            \n",
    "            # Train CatBoost model with GPU support\n",
    "            catboost_model = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, task_type=\"GPU\", random_seed=42, verbose=100)\n",
    "            catboost_model.fit(train_features, train_fold['label'])\n",
    "            \n",
    "            # Validate the model\n",
    "            val_preds = catboost_model.predict(val_features)\n",
    "            val_accuracy = accuracy_score(val_fold['label'], val_preds)\n",
    "            print(f\"Validation accuracy for fold {fold+1}: {val_accuracy}\")\n",
    "\n",
    "            # Check if this fold's accuracy is the best so far\n",
    "            if val_accuracy > best_accuracy_across_folds:\n",
    "                best_accuracy_across_folds = val_accuracy\n",
    "                # Save the model here if you want, e.g., using joblib or pickle\n",
    "                catboost_model.save_model(best_model_path)\n",
    "                print(f\"New best model for {category} - {attr} with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "        print(f\"Best model across all folds for {category} - {attr} with accuracy: {best_accuracy_across_folds}\")\n",
    "\n",
    "# ---- Prediction on Test Set ----\n",
    "all_predictions = []\n",
    "\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_test_category = df_test[df_test['Category'] == category]\n",
    "    df_test_category['id'] = df_test_category['id'].astype(str)\n",
    "    df_test_category['filename'] = df_test_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "    \n",
    "    for attr in attributes:\n",
    "        print(f\"Predicting on test set for category: {category}, attribute: {attr}\")\n",
    "        \n",
    "        # Load the best model (if saved)\n",
    "        catboost_model = CatBoostClassifier()\n",
    "        catboost_model.load_model(best_model_path)\n",
    "        \n",
    "        # Extract features for the test set\n",
    "        test_features = extract_features(df_test_category[['filename']], test_image_dir, feature_extractor)\n",
    "        \n",
    "        # Make predictions\n",
    "        test_preds = catboost_model.predict(test_features)\n",
    "        \n",
    "        # Convert labels back to original categories\n",
    "        test_preds_decoded = label_encoders[category][attr].inverse_transform(test_preds)\n",
    "        \n",
    "        # Ensure alignment between predictions and the test dataframe\n",
    "        df_test_category[f'predicted_{attr}'] = test_preds_decoded\n",
    "        \n",
    "        # Append the predictions to the main dataframe\n",
    "        all_predictions.append(df_test_category[['id', f'predicted_{attr}']])\n",
    "\n",
    "# Concatenate all the predictions and save to a CSV\n",
    "df_predictions = pd.concat(all_predictions)\n",
    "df_predictions.to_csv('Kurtis_attr_9.csv', index=False)\n",
    "\n",
    "print(\"Test predictions saved to 'Kurtis_attr_9.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
