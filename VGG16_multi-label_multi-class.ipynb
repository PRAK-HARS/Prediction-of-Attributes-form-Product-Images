{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras import backend as K\n",
    "from transformers import ViTImageProcessor, TFAutoModel\n",
    "\n",
    "df_train = pd.read_csv('/kaggle/input/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    'Kurtis': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9'],\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Custom F1 Score Metric\n",
    "def f1_metric(y_true, y_pred):\n",
    "    if len(y_true.shape) == 2 and y_true.shape[1] == 1:\n",
    "        y_pred = tf.round(y_pred)\n",
    "    else:\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        y_true = tf.argmax(y_true, axis=-1)\n",
    "    \n",
    "    true_positives = tf.reduce_sum(tf.cast(tf.equal(y_true, y_pred), tf.float32))\n",
    "    false_positives = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 0), tf.equal(y_pred, 1)), tf.float32))\n",
    "    false_negatives = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 1), tf.equal(y_pred, 0)), tf.float32))\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives + K.epsilon())\n",
    "    recall = true_positives / (true_positives + false_negatives + K.epsilon())\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1\n",
    "\n",
    "# Load a smaller pre-trained Vision Transformer\n",
    "\n",
    "# Extract features function using ViT\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Initialize VGG model with pretrained weights, excluding the top layers.\n",
    "vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Feature extraction function using VGG\n",
    "def extract_features_vgg(image_files, image_dir, feature_model, target_size=(224, 224)):\n",
    "    images = []\n",
    "    for filename in image_files['filename']:\n",
    "        img = tf.keras.preprocessing.image.load_img(f\"{image_dir}/{filename}\", target_size=target_size)\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = preprocess_input(img)  # Preprocess as VGG expects\n",
    "        images.append(img)\n",
    "    images = np.array(images)\n",
    "    features = feature_model.predict(images)  # Extract features with VGG\n",
    "    features = features.reshape(features.shape[0], -1)  # Flatten features\n",
    "    return features\n",
    "\n",
    "\n",
    "# Define the model for multi-output classification\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# Define the model for multi-output classification with custom layers per attribute\n",
    "def create_custom_multi_output_model(input_shape, num_classes_dict):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    outputs = {}\n",
    "    \n",
    "    # Custom Layers for attr_1\n",
    "    attr1_branch = layers.Dense(256, activation=None)(inputs)\n",
    "    attr1_branch = layers.BatchNormalization()(attr1_branch)\n",
    "    attr1_branch = layers.Dropout(0.6)(attr1_branch)\n",
    "    attr1_branch = layers.Dense(128, activation=None)(attr1_branch)\n",
    "    attr1_branch = layers.BatchNormalization()(attr1_branch)\n",
    "    attr1_branch = layers.Dropout(0.5)(attr1_branch)\n",
    "    outputs['attr_1'] = layers.Dense(1, activation='sigmoid', name='output_attr_1')(attr1_branch) if num_classes_dict['attr_1'] == 2 else layers.Dense(num_classes_dict['attr_1'], activation='softmax', name='output_attr_1')(attr1_branch)\n",
    "\n",
    "    # Custom Layers for attr_2\n",
    "    attr2_branch = layers.Dense(256, activation=None)(inputs)\n",
    "    attr2_branch = layers.Dropout(0.4)(attr2_branch)\n",
    "    attr2_branch = layers.Dense(64, activation=None)(attr2_branch)\n",
    "    attr2_branch = layers.BatchNormalization()(attr2_branch)\n",
    "    attr2_branch = layers.Dropout(0.3)(attr2_branch)\n",
    "    outputs['attr_2'] = layers.Dense(1, activation='sigmoid', name='output_attr_2')(attr2_branch) if num_classes_dict['attr_2'] == 2 else layers.Dense(num_classes_dict['attr_2'], activation='softmax', name='output_attr_2')(attr2_branch)\n",
    "\n",
    "    # Custom Layers for attr_3\n",
    "    attr3_branch = layers.Dense(256, activation=None)(inputs)\n",
    "    attr3_branch = layers.BatchNormalization()(attr3_branch)\n",
    "    attr3_branch = layers.Dropout(0.3)(attr3_branch)\n",
    "    attr3_branch = layers.Dense(128, activation=None)(attr3_branch)\n",
    "    attr3_branch = layers.BatchNormalization()(attr3_branch)\n",
    "    attr3_branch = layers.Dropout(0.2)(attr3_branch)\n",
    "    residual = layers.Dense(128, activation=None)(inputs)\n",
    "    residual = layers.BatchNormalization()(residual)\n",
    "    attr3_branch = layers.Add()([attr3_branch, residual])\n",
    "    attr3_branch = layers.ReLU()(attr3_branch)\n",
    "    outputs['attr_3'] = layers.Dense(1, activation='sigmoid', name='output_attr_3')(attr3_branch) if num_classes_dict['attr_3'] == 2 else layers.Dense(num_classes_dict['attr_3'], activation='softmax', name='output_attr_3')(attr3_branch)\n",
    "\n",
    "    # Custom Layers for attr_4\n",
    "    attr4_branch = layers.Dense(256, activation=None)(inputs)\n",
    "    attr4_branch = layers.ReLU()(attr4_branch)\n",
    "    attr4_branch = layers.Dropout(0.3)(attr4_branch)\n",
    "    attr4_branch = layers.Dense(128, activation=None)(attr4_branch)\n",
    "    attr4_branch = layers.Dropout(0.2)(attr4_branch)\n",
    "    outputs['attr_4'] = layers.Dense(1, activation='sigmoid', name='output_attr_4')(attr4_branch) if num_classes_dict['attr_4'] == 2 else layers.Dense(num_classes_dict['attr_4'], activation='softmax', name='output_attr_4')(attr4_branch)\n",
    "\n",
    "    # Custom Layers for attr_5\n",
    "    attr5_branch = layers.Dense(256, activation=None)(inputs)\n",
    "    attr5_branch = layers.BatchNormalization()(attr5_branch)\n",
    "    attr5_branch = layers.ReLU()(attr5_branch)\n",
    "    attr5_branch = layers.Dropout(0.3)(attr5_branch)\n",
    "    attr5_branch = layers.Dense(64, activation=None)(attr5_branch)\n",
    "    attr5_branch = layers.Dropout(0.2)(attr5_branch)\n",
    "    outputs['attr_5'] = layers.Dense(1, activation='sigmoid', name='output_attr_5')(attr5_branch) if num_classes_dict['attr_5'] == 2 else layers.Dense(num_classes_dict['attr_5'], activation='softmax', name='output_attr_5')(attr5_branch)\n",
    "\n",
    "    # Custom Layers for attr_6\n",
    "    attr6_branch = layers.Dense(256, activation=None)(inputs)\n",
    "    attr6_branch = layers.Dropout(0.3)(attr6_branch)\n",
    "    attr6_branch = layers.Dense(64, activation=None)(attr6_branch)\n",
    "    attr6_branch = layers.ReLU()(attr6_branch)\n",
    "    attr6_branch = layers.Dropout(0.2)(attr6_branch)\n",
    "    outputs['attr_6'] = layers.Dense(1, activation='sigmoid', name='output_attr_6')(attr6_branch) if num_classes_dict['attr_6'] == 2 else layers.Dense(num_classes_dict['attr_6'], activation='softmax', name='output_attr_6')(attr6_branch)\n",
    "\n",
    "    # Custom Layers for attr_7\n",
    "    attr7_branch = layers.Dense(256, activation=None)(inputs)\n",
    "    attr7_branch = layers.Dropout(0.3)(attr7_branch)\n",
    "    attr7_branch = layers.Dense(64, activation=None)(attr7_branch)\n",
    "    attr7_branch = layers.ReLU()(attr7_branch)\n",
    "    attr7_branch = layers.Dropout(0.2)(attr7_branch)\n",
    "    outputs['attr_7'] = layers.Dense(1, activation='sigmoid', name='output_attr_7')(attr7_branch) if num_classes_dict['attr_7'] == 2 else layers.Dense(num_classes_dict['attr_7'], activation='softmax', name='output_attr_7')(attr7_branch)\n",
    "\n",
    "    # Custom Layers for attr_8\n",
    "    attr8_branch = layers.Dense(256, activation=None)(inputs)\n",
    "    attr8_branch = layers.BatchNormalization()(attr8_branch)\n",
    "    attr8_branch = layers.Dropout(0.3)(attr8_branch)\n",
    "    attr8_branch = layers.Dense(128, activation=None)(attr8_branch)\n",
    "    attr8_branch = layers.BatchNormalization()(attr8_branch)\n",
    "    attr8_branch = layers.Dropout(0.3)(attr8_branch)\n",
    "    attr8_branch = layers.Dense(64, activation=None)(attr8_branch)\n",
    "    attr8_branch = layers.Dropout(0.2)(attr8_branch)\n",
    "    outputs['attr_8'] = layers.Dense(1, activation='sigmoid', name='output_attr_8')(attr8_branch) if num_classes_dict['attr_8'] == 2 else layers.Dense(num_classes_dict['attr_8'], activation='softmax', name='output_attr_8')(attr8_branch)\n",
    "\n",
    "    # Custom Layers for attr_9\n",
    "    attr9_branch = layers.Dense(256, activation=None)(inputs)\n",
    "    attr9_branch = layers.Dropout(0.3)(attr9_branch)\n",
    "    attr9_branch = layers.Dense(64, activation=None)(attr9_branch)\n",
    "    attr9_branch = layers.ReLU()(attr9_branch)\n",
    "    attr9_branch = layers.Dropout(0.2)(attr9_branch)\n",
    "    outputs['attr_9'] = layers.Dense(1, activation='sigmoid', name='output_attr_9')(attr9_branch) if num_classes_dict['attr_9'] == 2 else layers.Dense(num_classes_dict['attr_9'], activation='softmax', name='output_attr_9')(attr9_branch)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = models.Model(inputs=inputs, outputs=list(outputs.values()))\n",
    "    metrics = [f1_metric for _ in range(len(num_classes_dict))]\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "        loss=['binary_crossentropy' if num_classes == 2 else 'sparse_categorical_crossentropy' for num_classes in num_classes_dict.values()],\n",
    "        metrics= metrics\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Training and prediction\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    # Prepare label encoders for each attribute\n",
    "    df_attr = df_category[['filename'] + attributes].dropna()\n",
    "    num_classes_dict = {}\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    for attr in attributes:\n",
    "        le = LabelEncoder()\n",
    "        df_attr[attr] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "        num_classes_dict[attr] = len(le.classes_)\n",
    "\n",
    "    # Set up K-Fold Cross-Validation\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    best_f1_across_folds = 0\n",
    "    best_model_path = f'best_multi_output_model_{category}.h5'\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "        print(f\"Training fold {fold+1} for {category}\")\n",
    "\n",
    "        train_fold = df_attr.iloc[train_idx]\n",
    "        val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "        # Extract features for train and validation sets\n",
    "        train_features = extract_features_vgg(train_fold[['filename']], train_image_dir, vgg_model)\n",
    "        val_features = extract_features_vgg(val_fold[['filename']], train_image_dir, vgg_model)\n",
    "\n",
    "        # Prepare labels as a dictionary of targets\n",
    "        train_labels = {f'output_{attr}': train_fold[attr].values for attr in attributes}\n",
    "        val_labels = {f'output_{attr}': val_fold[attr].values for attr in attributes}\n",
    "\n",
    "        # Create and train the model\n",
    "        multi_output_model = create_custom_multi_output_model(train_features.shape[1:], num_classes_dict)\n",
    "        multi_output_model.fit(train_features, train_labels, epochs=10, batch_size=16, validation_data=(val_features, val_labels))\n",
    "\n",
    "        # Predict on validation data\n",
    "        val_preds = multi_output_model.predict(val_features)\n",
    "        fold_f1_scores = []\n",
    "        for i, attr in enumerate(attributes):\n",
    "            true_labels = val_labels[f'output_{attr}']\n",
    "            pred_labels = np.round(val_preds[i]).astype(int) if num_classes_dict[attr] == 2 else val_preds[i].argmax(axis=1)\n",
    "            fold_f1_scores.append(f1_score(true_labels, pred_labels, average='macro'))\n",
    "\n",
    "        # Calculate the mean F1-score for this fold\n",
    "        fold_f1 = np.mean(fold_f1_scores)\n",
    "        print(f\"Mean F1-score for fold {fold+1}: {fold_f1}\")\n",
    "\n",
    "        if fold_f1 > best_f1_across_folds:\n",
    "            best_f1_across_folds = fold_f1\n",
    "            multi_output_model.save(best_model_path)\n",
    "            print(f\"New best model for {category} with F1-score: {best_f1_across_folds}\")\n",
    "\n",
    "    print(f\"Best F1-score across all folds for {category}: {best_f1_across_folds}\")\n",
    "\n",
    "# ---- Prediction on Test Set ----\n",
    "all_predictions = []\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_test_category = df_test[df_test['Category'] == category]\n",
    "    df_test_category['id'] = df_test_category['id'].astype(str)\n",
    "    df_test_category['filename'] = df_test_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    # Load the best model for this category\n",
    "    multi_output_model = models.load_model(best_model_path)\n",
    "\n",
    "    # Extract features for the test set\n",
    "    test_features = extract_features_vgg(df_test_category[['filename']], test_image_dir, vgg_model)\n",
    "\n",
    "    # Predict for each attribute\n",
    "    test_preds = multi_output_model.predict(test_features)\n",
    "    for i, attr in enumerate(attributes):\n",
    "        pred_labels = np.round(test_preds[i]).astype(int) if num_classes_dict[attr] == 2 else test_preds[i].argmax(axis=1)\n",
    "        test_preds_decoded = label_encoders[category][attr].inverse_transform(pred_labels)\n",
    "        df_test_category[f'predicted_{attr}'] = test_preds_decoded\n",
    "\n",
    "    all_predictions.append(df_test_category[['id'] + [f'predicted_{attr}' for attr in attributes]])\n",
    "\n",
    "# Concatenate all predictions and save to CSV\n",
    "df_predictions = pd.concat(all_predictions)\n",
    "df_predictions.to_csv('multi_output_predictions.csv', index=False)\n",
    "\n",
    "print(\"Test predictions saved to 'multi_output_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras import backend as K\n",
    "from transformers import ViTImageProcessor, TFAutoModel\n",
    "\n",
    "df_train = pd.read_csv('/kaggle/input/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    'Mens Tshirt': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5'],\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Custom F1 Score Metric\n",
    "def f1_metric(y_true, y_pred):\n",
    "    if len(y_true.shape) == 2 and y_true.shape[1] == 1:\n",
    "        y_pred = tf.round(y_pred)\n",
    "    else:\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        y_true = tf.argmax(y_true, axis=-1)\n",
    "    \n",
    "    true_positives = tf.reduce_sum(tf.cast(tf.equal(y_true, y_pred), tf.float32))\n",
    "    false_positives = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 0), tf.equal(y_pred, 1)), tf.float32))\n",
    "    false_negatives = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 1), tf.equal(y_pred, 0)), tf.float32))\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives + K.epsilon())\n",
    "    recall = true_positives / (true_positives + false_negatives + K.epsilon())\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1\n",
    "\n",
    "# Load a smaller pre-trained Vision Transformer\n",
    "\n",
    "# Extract features function using ViT\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Initialize VGG model with pretrained weights, excluding the top layers.\n",
    "vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Feature extraction function using VGG\n",
    "def extract_features_vgg(image_files, image_dir, feature_model, target_size=(224, 224)):\n",
    "    images = []\n",
    "    for filename in image_files['filename']:\n",
    "        img = tf.keras.preprocessing.image.load_img(f\"{image_dir}/{filename}\", target_size=target_size)\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = preprocess_input(img)  # Preprocess as VGG expects\n",
    "        images.append(img)\n",
    "    images = np.array(images)\n",
    "    features = feature_model.predict(images)  # Extract features with VGG\n",
    "    features = features.reshape(features.shape[0], -1)  # Flatten features\n",
    "    return features\n",
    "\n",
    "\n",
    "# Define the model for multi-output classification\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# Define the model for multi-output classification with custom layers per attribute\n",
    "def create_custom_multi_output_model(input_shape, num_classes_dict):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    outputs = {}\n",
    "    \n",
    "    # Custom Layers for attr_1\n",
    "#     attr1_branch = layers.Dense(256, activation=None)(inputs)\n",
    "#     attr1_branch = layers.Dropout(0.5)(attr1_branch)\n",
    "    \n",
    "    attr1_branch = layers.Dense(128, activation=None)(attr1_branch)\n",
    "    attr1_branch = layers.BatchNormalization()(attr1_branch)\n",
    "    attr1_branch = layers.ReLU()(attr1_branch)\n",
    "    attr1_branch = layers.Dense(64, activation=None)(attr1_branch)\n",
    "    attr1_branch = layers.BatchNormalization()(attr1_branch)\n",
    "    attr1_branch = layers.Dropout(0.5)(attr1_branch)\n",
    "    \n",
    "    outputs['attr_1'] = layers.Dense(1, activation='sigmoid', name='output_attr_1')(attr1_branch) if num_classes_dict['attr_1'] == 2 else layers.Dense(num_classes_dict['attr_1'], activation='softmax', name='output_attr_1')(attr1_branch)\n",
    "\n",
    "    # Custom Layers for attr_2\n",
    "    attr2_branch = layers.Dense(256, activation=None)(inputs)\n",
    "    attr2_branch = layers.BatchNormalization()(attr2_branch)\n",
    "    attr2_branch = layers.Dropout(0.5)(attr2_branch)\n",
    "    outputs['attr_2'] = layers.Dense(1, activation='sigmoid', name='output_attr_2')(attr2_branch) if num_classes_dict['attr_2'] == 2 else layers.Dense(num_classes_dict['attr_2'], activation='softmax', name='output_attr_2')(attr2_branch)\n",
    "\n",
    "    # Custom Layers for attr_3\n",
    "    attr3_branch = layers.Dense(256, activation=None)(inputs)\n",
    "    attr3_branch = layers.Dropout(0.5)(attr3_branch)\n",
    "    \n",
    "    attr3_branch = layers.Dense(128, activation=None)(attr3_branch)\n",
    "    attr3_branch = layers.BatchNormalization()(attr3_branch)\n",
    "    attr3_branch = layers.ReLU()(attr3_branch)\n",
    "    attr3_branch = layers.Dropout(0.5)(attr3_branch)\n",
    "    \n",
    "    outputs['attr_3'] = layers.Dense(1, activation='sigmoid', name='output_attr_3')(attr3_branch) if num_classes_dict['attr_3'] == 2 else layers.Dense(num_classes_dict['attr_3'], activation='softmax', name='output_attr_3')(attr3_branch)\n",
    "\n",
    "    # Custom Layers for attr_4\n",
    "    attr4_branch = layers.Dense(256, activation=None)(inputs)\n",
    "    attr4_branch = layers.BatchNormalization()(attr4_branch)\n",
    "    attr4_branch = layers.ReLU()(attr4_branch)\n",
    "    attr4_branch = layers.Dropout(0.3)(attr4_branch)\n",
    "    outputs['attr_4'] = layers.Dense(1, activation='sigmoid', name='output_attr_4')(attr4_branch) if num_classes_dict['attr_4'] == 2 else layers.Dense(num_classes_dict['attr_4'], activation='softmax', name='output_attr_4')(attr4_branch)\n",
    "\n",
    "    # Custom Layers for attr_5\n",
    "    attr5_branch = layers.Dense(256, activation=None)(inputs)\n",
    "    attr5_branch = layers.BatchNormalization()(attr5_branch)\n",
    "    attr5_branch = layers.ReLU()(attr5_branch)\n",
    "    \n",
    "    attr5_branch = layers.Dense(128, activation=None)(attr5_branch)\n",
    "    attr5_branch = layers.BatchNormalization()(attr5_branch)\n",
    "    outputs['attr_5'] = layers.Dense(1, activation='sigmoid', name='output_attr_5')(attr5_branch) if num_classes_dict['attr_5'] == 2 else layers.Dense(num_classes_dict['attr_5'], activation='softmax', name='output_attr_5')(attr5_branch)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = models.Model(inputs=inputs, outputs=list(outputs.values()))\n",
    "    metrics = [f1_metric for _ in range(len(num_classes_dict))]\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "        loss=['binary_crossentropy' if num_classes == 2 else 'sparse_categorical_crossentropy' for num_classes in num_classes_dict.values()],\n",
    "        metrics= metrics\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Training and prediction\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    # Prepare label encoders for each attribute\n",
    "    df_attr = df_category[['filename'] + attributes].dropna()\n",
    "    num_classes_dict = {}\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    for attr in attributes:\n",
    "        le = LabelEncoder()\n",
    "        df_attr[attr] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "        num_classes_dict[attr] = len(le.classes_)\n",
    "\n",
    "    # Set up K-Fold Cross-Validation\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    best_f1_across_folds = 0\n",
    "    best_model_path = f'best_multi_output_model_{category}.h5'\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "        print(f\"Training fold {fold+1} for {category}\")\n",
    "\n",
    "        train_fold = df_attr.iloc[train_idx]\n",
    "        val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "        # Extract features for train and validation sets\n",
    "        train_features = extract_features_vgg(train_fold[['filename']], train_image_dir, vgg_model)\n",
    "        val_features = extract_features_vgg(val_fold[['filename']], train_image_dir, vgg_model)\n",
    "\n",
    "        # Prepare labels as a dictionary of targets\n",
    "        train_labels = {f'output_{attr}': train_fold[attr].values for attr in attributes}\n",
    "        val_labels = {f'output_{attr}': val_fold[attr].values for attr in attributes}\n",
    "\n",
    "        # Create and train the model\n",
    "        multi_output_model = create_custom_multi_output_model(train_features.shape[1:], num_classes_dict)\n",
    "        multi_output_model.fit(train_features, train_labels, epochs=10, batch_size=16, validation_data=(val_features, val_labels))\n",
    "\n",
    "        # Predict on validation data\n",
    "        val_preds = multi_output_model.predict(val_features)\n",
    "        fold_f1_scores = []\n",
    "        for i, attr in enumerate(attributes):\n",
    "            true_labels = val_labels[f'output_{attr}']\n",
    "            pred_labels = np.round(val_preds[i]).astype(int) if num_classes_dict[attr] == 2 else val_preds[i].argmax(axis=1)\n",
    "            fold_f1_scores.append(f1_score(true_labels, pred_labels, average='macro'))\n",
    "\n",
    "        # Calculate the mean F1-score for this fold\n",
    "        fold_f1 = np.mean(fold_f1_scores)\n",
    "        print(f\"Mean F1-score for fold {fold+1}: {fold_f1}\")\n",
    "\n",
    "        if fold_f1 > best_f1_across_folds:\n",
    "            best_f1_across_folds = fold_f1\n",
    "            multi_output_model.save(best_model_path)\n",
    "            print(f\"New best model for {category} with F1-score: {best_f1_across_folds}\")\n",
    "\n",
    "    print(f\"Best F1-score across all folds for {category}: {best_f1_across_folds}\")\n",
    "\n",
    "# ---- Prediction on Test Set ----\n",
    "all_predictions = []\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_test_category = df_test[df_test['Category'] == category]\n",
    "    df_test_category['id'] = df_test_category['id'].astype(str)\n",
    "    df_test_category['filename'] = df_test_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    # Load the best model for this category\n",
    "    multi_output_model = models.load_model(best_model_path)\n",
    "\n",
    "    # Extract features for the test set\n",
    "    test_features = extract_features_vgg(df_test_category[['filename']], test_image_dir, vgg_model)\n",
    "\n",
    "    # Predict for each attribute\n",
    "    test_preds = multi_output_model.predict(test_features)\n",
    "    for i, attr in enumerate(attributes):\n",
    "        pred_labels = np.round(test_preds[i]).astype(int) if num_classes_dict[attr] == 2 else test_preds[i].argmax(axis=1)\n",
    "        test_preds_decoded = label_encoders[category][attr].inverse_transform(pred_labels)\n",
    "        df_test_category[f'predicted_{attr}'] = test_preds_decoded\n",
    "\n",
    "    all_predictions.append(df_test_category[['id'] + [f'predicted_{attr}' for attr in attributes]])\n",
    "\n",
    "# Concatenate all predictions and save to CSV\n",
    "df_predictions = pd.concat(all_predictions)\n",
    "df_predictions.to_csv('multi_output_predictions.csv', index=False)\n",
    "\n",
    "print(\"Test predictions saved to 'multi_output_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras import backend as K\n",
    "from transformers import ViTImageProcessor, TFAutoModel\n",
    "\n",
    "df_train = pd.read_csv('/kaggle/input/visual-taxonomy/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/visual-taxonomy/test.csv')\n",
    "train_image_dir = '/kaggle/input/visual-taxonomy/train_images'\n",
    "test_image_dir = '/kaggle/input/visual-taxonomy/test_images'\n",
    "\n",
    "# Define attribute configurations for each category\n",
    "categories_attributes = {\n",
    "    'Sarees': ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10'],\n",
    "}\n",
    "\n",
    "# Dictionary to store encoders for each category and attribute\n",
    "label_encoders = {}\n",
    "\n",
    "# Custom F1 Score Metric\n",
    "def f1_metric(y_true, y_pred):\n",
    "    if len(y_true.shape) == 2 and y_true.shape[1] == 1:\n",
    "        y_pred = tf.round(y_pred)\n",
    "    else:\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        y_true = tf.argmax(y_true, axis=-1)\n",
    "    \n",
    "    true_positives = tf.reduce_sum(tf.cast(tf.equal(y_true, y_pred), tf.float32))\n",
    "    false_positives = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 0), tf.equal(y_pred, 1)), tf.float32))\n",
    "    false_negatives = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 1), tf.equal(y_pred, 0)), tf.float32))\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives + K.epsilon())\n",
    "    recall = true_positives / (true_positives + false_negatives + K.epsilon())\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1\n",
    "\n",
    "# Load a smaller pre-trained Vision Transformer\n",
    "\n",
    "# Extract features function using ViT\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Initialize VGG model with pretrained weights, excluding the top layers.\n",
    "vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Feature extraction function using VGG\n",
    "def extract_features_vgg(image_files, image_dir, feature_model, target_size=(224, 224)):\n",
    "    images = []\n",
    "    for filename in image_files['filename']:\n",
    "        img = tf.keras.preprocessing.image.load_img(f\"{image_dir}/{filename}\", target_size=target_size)\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = preprocess_input(img)  # Preprocess as VGG expects\n",
    "        images.append(img)\n",
    "    images = np.array(images)\n",
    "    features = feature_model.predict(images)  # Extract features with VGG\n",
    "    features = features.reshape(features.shape[0], -1)  # Flatten features\n",
    "    return features\n",
    "\n",
    "\n",
    "# Define the model for multi-output classification\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# Define the model for multi-output classification with custom layers per attribute\n",
    "def create_custom_multi_output_model(input_shape, num_classes_dict):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    outputs = {}\n",
    "    \n",
    "    # Custom Layers for attr_1\n",
    "#     attr1_branch = layers.Dense(256, activation=None)(inputs)\n",
    "#     attr1_branch = layers.Dropout(0.5)(attr1_branch)\n",
    "    \n",
    "    attr1_branch = layers.Dense(128, activation=None)(attr1_branch)\n",
    "    attr1_branch = layers.BatchNormalization()(attr1_branch)\n",
    "    attr1_branch = layers.ReLU()(attr1_branch)\n",
    "    attr1_branch = layers.ReLU()(attr1_branch)\n",
    "    attr1_branch = layers.Dropout(0.5)(attr1_branch)\n",
    "    \n",
    "    outputs['attr_1'] = layers.Dense(1, activation='sigmoid', name='output_attr_1')(attr1_branch) if num_classes_dict['attr_1'] == 2 else layers.Dense(num_classes_dict['attr_1'], activation='softmax', name='output_attr_1')(attr1_branch)\n",
    "\n",
    "    # Custom Layers for attr_2\n",
    "    attr2_branch = layers.Dense(256, activation=None)(inputs)\n",
    "    attr2_branch = layers.BatchNormalization()(attr2_branch)\n",
    "    attr2_branch = layers.ReLU()(attr2_branch)\n",
    "    attr2_branch = layers.Dropout(0.5)(attr2_branch)\n",
    "    outputs['attr_2'] = layers.Dense(1, activation='sigmoid', name='output_attr_2')(attr2_branch) if num_classes_dict['attr_2'] == 2 else layers.Dense(num_classes_dict['attr_2'], activation='softmax', name='output_attr_2')(attr2_branch)\n",
    "\n",
    "    # Custom Layers for attr_3\n",
    "    attr3_branch = layers.Dense(256, activation=None)(inputs)\n",
    "    attr3_branch = layers.Dropout(0.5)(attr3_branch)\n",
    "    \n",
    "    attr3_branch = layers.Dense(128, activation=None)(attr3_branch)\n",
    "    attr3_branch = layers.BatchNormalization()(attr3_branch)\n",
    "    attr3_branch = layers.ReLU()(attr3_branch)\n",
    "    attr3_branch = layers.Dropout(0.5)(attr3_branch)\n",
    "    \n",
    "    outputs['attr_3'] = layers.Dense(1, activation='sigmoid', name='output_attr_3')(attr3_branch) if num_classes_dict['attr_3'] == 2 else layers.Dense(num_classes_dict['attr_3'], activation='softmax', name='output_attr_3')(attr3_branch)\n",
    "\n",
    "    # Custom Layers for attr_4\n",
    "    attr4_branch = layers.Dense(256, activation=None)(inputs)\n",
    "    attr4_branch = layers.BatchNormalization()(attr4_branch)\n",
    "    attr4_branch = layers.ReLU()(attr4_branch)\n",
    "    attr4_branch = layers.Dropout(0.3)(attr4_branch)\n",
    "    outputs['attr_4'] = layers.Dense(1, activation='sigmoid', name='output_attr_4')(attr4_branch) if num_classes_dict['attr_4'] == 2 else layers.Dense(num_classes_dict['attr_4'], activation='softmax', name='output_attr_4')(attr4_branch)\n",
    "\n",
    "    # Custom Layers for attr_5\n",
    "    attr5_branch = layers.Dense(256, activation=None)(inputs)\n",
    "    attr5_branch = layers.BatchNormalization()(attr5_branch)\n",
    "    attr5_branch = layers.ReLU()(attr5_branch)\n",
    "    attr5_branch = layers.Dense(128, activation=None)(attr5_branch)\n",
    "    outputs['attr_5'] = layers.Dense(1, activation='sigmoid', name='output_attr_5')(attr5_branch) if num_classes_dict['attr_5'] == 2 else layers.Dense(num_classes_dict['attr_5'], activation='softmax', name='output_attr_5')(attr5_branch)\n",
    "\n",
    "    # Custom Layers for attr_6\n",
    "    attr6_branch = layers.Dense(256, activation=None)(inputs)\n",
    "    attr6_branch = layers.BatchNormalization()(attr6_branch)\n",
    "    attr6_branch = layers.ReLU()(attr6_branch)\n",
    "    attr6_branch = layers.Dropout(0.5)(attr6_branch)\n",
    "    outputs['attr_6'] = layers.Dense(1, activation='sigmoid', name='output_attr_6')(attr6_branch) if num_classes_dict['attr_6'] == 2 else layers.Dense(num_classes_dict['attr_6'], activation='softmax', name='output_attr_6')(attr6_branch)\n",
    "\n",
    "    # Custom Layers for attr_7\n",
    "    attr7_branch = layers.Dense(256, activation=None)(inputs)\n",
    "    attr7_branch = layers.BatchNormalization()(attr7_branch)\n",
    "    attr7_branch = layers.ReLU()(attr7_branch)\n",
    "    attr7_branch = layers.Dropout(0.5)(attr7_branch)\n",
    "    outputs['attr_7'] = layers.Dense(1, activation='sigmoid', name='output_attr_7')(attr7_branch) if num_classes_dict['attr_7'] == 2 else layers.Dense(num_classes_dict['attr_7'], activation='softmax', name='output_attr_7')(attr7_branch)\n",
    "\n",
    "    # Custom Layers for attr_8\n",
    "    attr8_branch = layers.Dense(256, activation=None)(inputs)\n",
    "    attr8_branch = layers.BatchNormalization()(attr8_branch)\n",
    "    outputs['attr_8'] = layers.Dense(1, activation='sigmoid', name='output_attr_8')(attr8_branch) if num_classes_dict['attr_8'] == 2 else layers.Dense(num_classes_dict['attr_8'], activation='softmax', name='output_attr_8')(attr8_branch)\n",
    "\n",
    "   # custom layers for attr_9\n",
    "    attr9_branch = layers.Dense(256, activation=None)(inputs)\n",
    "    attr9_branch = layers.BatchNormalization()(attr9_branch)\n",
    "    attr9_branch = layers.Dropout(0.5)(attr9_branch)\n",
    "    outputs['attr_9'] = layers.Dense(1, activation='sigmoid', name='output_attr_9')(attr9_branch) if num_classes_dict['attr_9'] == 2 else layers.Dense(num_classes_dict['attr_9'], activation='softmax', name='output_attr_9')(attr9_branch)\n",
    "\n",
    "   # custom layer for attr_10 \n",
    "    attr10_branch = layers.Dense(256, activation=None)(inputs)\n",
    "    attr10_branch = layers.BatchNormalization()(attr10_branch)\n",
    "    attr10_branch = layers.Dropout(0.5)(attr10_branch)\n",
    "    outputs['attr_10'] = layers.Dense(1, activation='sigmoid', name='output_attr_10')(attr10_branch) if num_classes_dict['attr_10'] == 2 else layers.Dense(num_classes_dict['attr_10'], activation='softmax', name='output_attr_10')(attr10_branch)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = models.Model(inputs=inputs, outputs=list(outputs.values()))\n",
    "    metrics = [f1_metric for _ in range(len(num_classes_dict))]\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "        loss=['binary_crossentropy' if num_classes == 2 else 'sparse_categorical_crossentropy' for num_classes in num_classes_dict.values()],\n",
    "        metrics= metrics\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Training and prediction\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_category = df_train[df_train['Category'] == category]\n",
    "    df_category['id'] = df_category['id'].astype(str)\n",
    "    df_category['filename'] = df_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    # Prepare label encoders for each attribute\n",
    "    df_attr = df_category[['filename'] + attributes].dropna()\n",
    "    num_classes_dict = {}\n",
    "    label_encoders[category] = {}\n",
    "\n",
    "    for attr in attributes:\n",
    "        le = LabelEncoder()\n",
    "        df_attr[attr] = le.fit_transform(df_attr[attr])\n",
    "        label_encoders[category][attr] = le\n",
    "        num_classes_dict[attr] = len(le.classes_)\n",
    "\n",
    "    # Set up K-Fold Cross-Validation\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    best_f1_across_folds = 0\n",
    "    best_model_path = f'best_multi_output_model_{category}.h5'\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(df_attr)):\n",
    "        print(f\"Training fold {fold+1} for {category}\")\n",
    "\n",
    "        train_fold = df_attr.iloc[train_idx]\n",
    "        val_fold = df_attr.iloc[val_idx]\n",
    "\n",
    "        # Extract features for train and validation sets\n",
    "        train_features = extract_features_vgg(train_fold[['filename']], train_image_dir, vgg_model)\n",
    "        val_features = extract_features_vgg(val_fold[['filename']], train_image_dir, vgg_model)\n",
    "\n",
    "        # Prepare labels as a dictionary of targets\n",
    "        train_labels = {f'output_{attr}': train_fold[attr].values for attr in attributes}\n",
    "        val_labels = {f'output_{attr}': val_fold[attr].values for attr in attributes}\n",
    "\n",
    "        # Create and train the model\n",
    "        multi_output_model = create_custom_multi_output_model(train_features.shape[1:], num_classes_dict)\n",
    "        multi_output_model.fit(train_features, train_labels, epochs=10, batch_size=16, validation_data=(val_features, val_labels))\n",
    "\n",
    "        # Predict on validation data\n",
    "        val_preds = multi_output_model.predict(val_features)\n",
    "        fold_f1_scores = []\n",
    "        for i, attr in enumerate(attributes):\n",
    "            true_labels = val_labels[f'output_{attr}']\n",
    "            pred_labels = np.round(val_preds[i]).astype(int) if num_classes_dict[attr] == 2 else val_preds[i].argmax(axis=1)\n",
    "            fold_f1_scores.append(f1_score(true_labels, pred_labels, average='macro'))\n",
    "\n",
    "        # Calculate the mean F1-score for this fold\n",
    "        fold_f1 = np.mean(fold_f1_scores)\n",
    "        print(f\"Mean F1-score for fold {fold+1}: {fold_f1}\")\n",
    "\n",
    "        if fold_f1 > best_f1_across_folds:\n",
    "            best_f1_across_folds = fold_f1\n",
    "            multi_output_model.save(best_model_path)\n",
    "            print(f\"New best model for {category} with F1-score: {best_f1_across_folds}\")\n",
    "\n",
    "    print(f\"Best F1-score across all folds for {category}: {best_f1_across_folds}\")\n",
    "\n",
    "# ---- Prediction on Test Set ----\n",
    "all_predictions = []\n",
    "for category, attributes in categories_attributes.items():\n",
    "    df_test_category = df_test[df_test['Category'] == category]\n",
    "    df_test_category['id'] = df_test_category['id'].astype(str)\n",
    "    df_test_category['filename'] = df_test_category['id'].apply(lambda x: x.zfill(6) + '.jpg')\n",
    "\n",
    "    # Load the best model for this category\n",
    "    multi_output_model = models.load_model(best_model_path)\n",
    "\n",
    "    # Extract features for the test set\n",
    "    test_features = extract_features_vgg(df_test_category[['filename']], test_image_dir, vgg_model)\n",
    "\n",
    "    # Predict for each attribute\n",
    "    test_preds = multi_output_model.predict(test_features)\n",
    "    for i, attr in enumerate(attributes):\n",
    "        pred_labels = np.round(test_preds[i]).astype(int) if num_classes_dict[attr] == 2 else test_preds[i].argmax(axis=1)\n",
    "        test_preds_decoded = label_encoders[category][attr].inverse_transform(pred_labels)\n",
    "        df_test_category[f'predicted_{attr}'] = test_preds_decoded\n",
    "\n",
    "    all_predictions.append(df_test_category[['id'] + [f'predicted_{attr}' for attr in attributes]])\n",
    "\n",
    "# Concatenate all predictions and save to CSV\n",
    "df_predictions = pd.concat(all_predictions)\n",
    "df_predictions.to_csv('multi_output_predictions.csv', index=False)\n",
    "\n",
    "print(\"Test predictions saved to 'multi_output_predictions.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
